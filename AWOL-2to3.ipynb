{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'isaw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6fa4ce4514ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# from pyzotero import zotero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0misaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mawol_article\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0misaw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawol_parsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAwolParsers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'isaw'"
     ]
    }
   ],
   "source": [
    "# walk_to_json.py post 2to3 refactoring\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script to walk AWOL backup and create json resource files.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import argparse\n",
    "import errno\n",
    "import fileinput\n",
    "from functools import wraps\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "# from pyzotero import zotero\n",
    "from isaw.awol import awol_article, resource\n",
    "from isaw.awol.parse.awol_parsers import AwolParsers\n",
    "\n",
    "RX_URLFLAT = re.compile(r'[=+\\?\\{\\}\\{\\}\\(\\)\\\\\\-_&%#/,\\.;:]+')\n",
    "RX_DEDUPEH = re.compile(r'[-]+')\n",
    "DEFAULTLOGLEVEL = logging.WARNING\n",
    "\n",
    "def arglogger(func):\n",
    "    \"\"\"\n",
    "    decorator to log argument calls to functions\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def inner(*args, **kwargs): \n",
    "        logger = logging.getLogger(func.__name__)\n",
    "        logger.debug(\"called with arguments: %s, %s\" % (args, kwargs))\n",
    "        return func(*args, **kwargs) \n",
    "    return inner    \n",
    "\n",
    "\n",
    "@arglogger\n",
    "def main (args):\n",
    "    \"\"\"\n",
    "    main functions\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "    root_dir = args.whence[0]\n",
    "    dest_dir = args.thence[0]\n",
    "    walk_count = 0\n",
    "    resources = None\n",
    "    index = {}\n",
    "    parsers = AwolParsers()\n",
    "    for dir_name, sub_dir_list, file_list in os.walk(root_dir):\n",
    "        if resources is not None:\n",
    "            del resources\n",
    "        for file_name in file_list:\n",
    "            if 'post-' in file_name and file_name[-4:] == '.xml':\n",
    "                walk_count = walk_count + 1\n",
    "                if args.progress and walk_count % 50 == 1:\n",
    "                    print(('\\n*****************************\\nPERCENT COMPLETE: {0:.0f}\\n'.format(float(walk_count)/4261.0*100.0)))\n",
    "                logger.info('\\n=========================================================================================\\nARTICLE:\\n')\n",
    "                target = os.path.join(dir_name, file_name)\n",
    "                try:\n",
    "                    a = awol_article.AwolArticle(atom_file_name=target)\n",
    "                except (ValueError, RuntimeError) as e:\n",
    "                    logger.warning(e)\n",
    "                else:\n",
    "                    logger.info('article title: {0}'.format(a.title))\n",
    "                    logger.info('url: {0}'.format(a.url))\n",
    "                    awol_id = '-'.join(('awol', a.id.split('.')[-1]))\n",
    "                    logger.info('awol_id: {0}'.format(awol_id))\n",
    "                    resources = None\n",
    "                    try:\n",
    "                        resources = parsers.parse(a)\n",
    "                    except NotImplementedError as e:\n",
    "                        logger.warning(e)\n",
    "                    else:\n",
    "                        try:\n",
    "                            length = len(resources)\n",
    "                        except TypeError:\n",
    "                            length = 0\n",
    "                        if length > 0:\n",
    "                            for i,r in enumerate(resources):\n",
    "                                logger.info('\\n-----------------------------------------------------------------------------------------\\nRESOURCE\\n')\n",
    "                                logger.info('url: {0}'.format(r.url))\n",
    "                                logger.info('title: {0}'.format(r.title))\n",
    "                                domain = r.domain\n",
    "                                this_dir = os.path.join(dest_dir, domain)\n",
    "                                try:\n",
    "                                    os.makedirs(this_dir)\n",
    "                                except OSError as exc:\n",
    "                                    if exc.errno == errno.EEXIST and os.path.isdir(this_dir):\n",
    "                                        pass\n",
    "                                    else: raise\n",
    "                                try:\n",
    "                                    domain_index = index[domain]\n",
    "                                except KeyError:\n",
    "                                    domain_index = index[domain] = {}\n",
    "                                stub = r.url.split(domain)[-1][1:].encode('utf-8')\n",
    "                                if stub == '' or stub == '/':\n",
    "                                    stub = domain.encode('utf-8').replace('.', '-')\n",
    "                                if stub[-1] == '/':\n",
    "                                    stub = stub[:-1]\n",
    "                                if len(stub) > 80 or '?' in stub or '&' in stub or '%' in stub or ' ' in stub:\n",
    "                                    m = hashlib.sha1()\n",
    "                                    m.update(stub)\n",
    "                                    resource_key = m.hexdigest()\n",
    "                                else:\n",
    "                                    resource_key = RX_DEDUPEH.sub('-', RX_URLFLAT.sub('-', stub))\n",
    "                                filename = '.'.join((resource_key, 'json'))\n",
    "                                this_path = os.path.join(this_dir, filename)\n",
    "                                try:\n",
    "                                    domain_resources = domain_index[resource_key]\n",
    "                                except KeyError:\n",
    "                                    pass\n",
    "                                else:                    \n",
    "                                    # collision! load earlier version from disk and merge\n",
    "                                    logger.warning('collision in {0}: {1}/{2}'.format(a.url, r.domain, resource_key))\n",
    "                                    r_earlier = resource.Resource()\n",
    "                                    r_earlier.json_load(this_path)\n",
    "                                    try:\n",
    "                                        r_merged = resource.merge(r_earlier, r)\n",
    "                                    except ValueError as e:\n",
    "                                        logger.error(str(e) + ' while trying to merge; saving separately')\n",
    "                                        m = hashlib.sha1()\n",
    "                                        m.update(r.url)\n",
    "                                        resource_key = m.hexdigest()\n",
    "                                        filename = '.'.join((resource_key, 'json'))\n",
    "                                        this_path = os.path.join(this_dir, filename)\n",
    "                                    else:\n",
    "                                        r = r_merged\n",
    "                                    del r_earlier\n",
    "                                r.resource_key = resource_key\n",
    "                                r.json_dump(this_path, formatted=True)\n",
    "                                logger.info('filename: {0}'.format(this_path))\n",
    "                                try:\n",
    "                                    resource_title = r.extended_title\n",
    "                                except AttributeError:\n",
    "                                    resource_title = r.title\n",
    "                                resource_package = {\n",
    "                                    'title_full': resource_title,\n",
    "                                    'url': r.url,\n",
    "                                    'key': resource_key,\n",
    "                                }\n",
    "                                if resource_title != r.title:\n",
    "                                    resource_package['title'] = r.title\n",
    "                                try:\n",
    "                                    resource_list = domain_index[resource_key]\n",
    "                                except KeyError:\n",
    "                                    resource_list = domain_index[resource_key] = []\n",
    "                                resource_list.append(resource_package)\n",
    "            else:\n",
    "                logger.debug('skipping {0}'.format(file_name))\n",
    "        for ignore_dir in ['.git', '.svn', '.hg']:\n",
    "            if ignore_dir in sub_dir_list:\n",
    "                sub_dir_list.remove(ignore_dir)\n",
    "\n",
    "    logger.info('sorting domain list')\n",
    "    domain_list = sorted(index.keys())\n",
    "    domain_count = len(domain_list)\n",
    "    resource_count = 0\n",
    "    record_count = 0\n",
    "    max_collisions = 0\n",
    "    total_collisions = 0\n",
    "    redundant_resources = 0\n",
    "    logger.info(\"FULL INDEX OF RESOURCES\")\n",
    "    logger.info(\"=======================\")\n",
    "    for domain in domain_list:\n",
    "        logger.info(domain)\n",
    "        i = 0\n",
    "        dash = ''\n",
    "        while i < len(domain):\n",
    "            dash = dash+'-'\n",
    "            i = i+1\n",
    "        logger.info(dash)\n",
    "        logger.info('sorting resource list for domain {0}'.format(domain))\n",
    "        resource_list = sorted(index[domain].keys())\n",
    "        logger.info('{0} unique resources in this domain'.format(len(resource_list)))\n",
    "        resource_count = resource_count + len(resource_list)\n",
    "        for resource_key in resource_list:\n",
    "            resources = index[domain][resource_key]\n",
    "            logger.info('    {0}'.format(resources[0]['title_full']))\n",
    "            record_count = record_count + len(resources)\n",
    "            if len(resources) > 1:\n",
    "                logger.info ('        multiple records: {0}'.format(len(resources)))\n",
    "                total_collisions = total_collisions + len(resources)\n",
    "                redundant_resources = redundant_resources + 1\n",
    "                if len(resources) > max_collisions:\n",
    "                    max_collisions = len(resources)\n",
    "    logger.info(\"=======================\")\n",
    "    logger.info(\"Total {0} domains\".format(domain_count))\n",
    "    logger.info(\"Total {0} unique resources recorded\".format(resource_count))\n",
    "    logger.info(\"Total number of records: {0}\".format(record_count))\n",
    "    logger.info(\"Highest number of redundancies (collisions): {0}\".format(max_collisions))\n",
    "    logger.info(\"Total number of redundant records: {0}\".format(total_collisions))\n",
    "    logger.info(\"Percentage of redundantly recorded resources: {0:.2f}\".format(round(float(redundant_resources)/float(resource_count)*100.0),2))\n",
    "if __name__ == \"__main__\":\n",
    "    log_level = DEFAULTLOGLEVEL\n",
    "    log_level_name = logging.getLevelName(log_level)\n",
    "    logging.basicConfig(level=log_level)\n",
    "\n",
    "    try:\n",
    "        parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "        parser.add_argument (\"-l\", \"--loglevel\", type=str, help=\"desired logging level (case-insensitive string: DEBUG, INFO, WARNING, ERROR\" )\n",
    "        parser.add_argument (\"-v\", \"--verbose\", action=\"store_true\", default=False, help=\"verbose output (logging level == INFO\")\n",
    "        parser.add_argument (\"-vv\", \"--veryverbose\", action=\"store_true\", default=False, help=\"very verbose output (logging level == DEBUG\")\n",
    "        parser.add_argument (\"--progress\", action=\"store_true\", default=False, help=\"show progress\")\n",
    "        parser.add_argument('credfile', type=str, nargs=1, help='path to credential file')\n",
    "        #parser.add_argument('postfile', type=str, nargs='?', help='filename containing list of post files to process')\n",
    "        parser.add_argument('whence', type=str, nargs=1, help='path to directory to read and process')\n",
    "        parser.add_argument('thence', type=str, nargs=1, help='path to directory where you want the json-serialized resources dumped')\n",
    "        args = parser.parse_args()\n",
    "        if args.loglevel is not None:\n",
    "            args_log_level = re.sub('\\s+', '', args.loglevel.strip().upper())\n",
    "            try:\n",
    "                log_level = getattr(logging, args_log_level)\n",
    "            except AttributeError:\n",
    "                logging.error(\"command line option to set log_level failed because '%s' is not a valid level name; using %s\" % (args_log_level, log_level_name))\n",
    "        if args.veryverbose:\n",
    "            log_level = logging.DEBUG\n",
    "        elif args.verbose:\n",
    "            log_level = logging.INFO\n",
    "        log_level_name = logging.getLevelName(log_level)\n",
    "        logging.getLogger().setLevel(log_level)\n",
    "        if log_level != DEFAULTLOGLEVEL:\n",
    "            logging.warning(\"logging level changed to %s via command line option\" % log_level_name)\n",
    "        else:\n",
    "            logging.info(\"using default logging level: %s\" % log_level_name)\n",
    "        logging.debug(\"command line: '%s'\" % ' '.join(sys.argv))\n",
    "        main(args)\n",
    "        sys.exit(0)\n",
    "    except KeyboardInterrupt as e: # Ctrl-C\n",
    "        raise e\n",
    "    except SystemExit as e: # sys.exit()\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(\"ERROR, UNEXPECTED EXCEPTION\")\n",
    "        print(str(e))\n",
    "        traceback.print_exc()\n",
    "        os._exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# awol_article.py post 2to3 refactoring\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Work with an Atom entry representing an AWOL blog post.\n",
    "\n",
    "This module defines the following classes:\n",
    " \n",
    " * AwolArticle: represents key information about the entry.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from importlib import import_module\n",
    "import logging\n",
    "import os\n",
    "import pkg_resources\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import langid\n",
    "import requests\n",
    "import unicodecsv\n",
    "\n",
    "from isaw.awol.article import Article\n",
    "from isaw.awol.normalize_space import normalize_space\n",
    "from isaw.awol.resource import Resource\n",
    "\n",
    "\n",
    "PATH_CURRENT = os.path.dirname(os.path.abspath(__file__))\n",
    "# Build a dictionary of format {<colon prefix>:<list of cols 2,3 and 4>}\n",
    "colon_prefix_csv = pkg_resources.resource_stream('isaw.awol', 'awol_colon_prefixes.csv')\n",
    "dreader = unicodecsv.DictReader(\n",
    "    colon_prefix_csv,\n",
    "    fieldnames = [\n",
    "        'col_pre', \n",
    "        'omit_post', \n",
    "        'strip_title', \n",
    "        'mul_res'\n",
    "    ], \n",
    "    delimiter = ',', \n",
    "    quotechar = '\"')\n",
    "COLON_PREFIXES = dict()\n",
    "for row in dreader:\n",
    "    COLON_PREFIXES.update({\n",
    "        normalize_space(row['col_pre']).lower():\n",
    "            [\n",
    "                row['omit_post'], \n",
    "                row['strip_title'], \n",
    "                row['mul_res']\n",
    "            ]\n",
    "    })\n",
    "del dreader\n",
    "DOMAINS_TO_IGNORE = [\n",
    "    'draft.blogger.com'\n",
    "]\n",
    "DOMAINS_SECONDARY = [\n",
    "    'ancientworldonline.blogspot.com'\n",
    "]\n",
    "LANGID_THRESHOLD = 0.95\n",
    "RX_CANARY = re.compile(r'[\\.,:!\\\"“„\\;\\-\\s]+', re.IGNORECASE)\n",
    "RX_NUMERICISH = re.compile(r'^a?n?d?\\s*[\\.,:!\\\"“„\\;\\-\\s\\d\\(\\)\\[\\]]+$', re.IGNORECASE)\n",
    "RX_MATCH_DOMAIN = re.compile('^https?:\\/\\/([^/#]+)')\n",
    "RX_IDENTIFIERS = {\n",
    "    'issn': {\n",
    "        'electronic': [\n",
    "            re.compile(r'(electronic|e-|e‒|e–|e—|e|online|on-line|digital)([\\s:]*issn[^\\d]*[\\dX-‒–—]{4}[-‒–—\\s]?[\\dX]{4})', re.IGNORECASE),\n",
    "            re.compile(r'(issn[\\s\\(]*)(electrónico|électronique|online|on-line|digital)([^\\d]*[\\dX-‒–—]{4}[-‒–—\\s]?[\\dX]{4})', re.IGNORECASE),\n",
    "            re.compile(r'(issn[^\\d]*[\\dX-‒–—]{4}[-‒–—\\s]?[\\dX]{4}[\\s\\(]*)(electrónico|électronique|online|on-line|digital)', re.IGNORECASE),\n",
    "        ],\n",
    "        'generic': [\n",
    "            re.compile(r'(issn[^\\d]*[\\dX-‒–—]{4}[-‒–—\\s]?[\\dX]{4})', re.IGNORECASE),\n",
    "            re.compile(r'(issn[^\\d]*[\\dX-‒–—]{8-9})', re.IGNORECASE)\n",
    "        ],\n",
    "        'extract': {\n",
    "            'precise': re.compile(r'^[^\\d]*([\\dX]{4}[-‒–—\\s]?[\\dX]{4}).*$', re.IGNORECASE),\n",
    "            'fallback': re.compile(r'^[^\\d]*([\\dX-‒–—\\s]+).*$', re.IGNORECASE)\n",
    "        }\n",
    "    },\n",
    "    'isbn': {\n",
    "        'electronic': [\n",
    "            re.compile(r'(electronic|e-|e‒|e–|e—|online|on-line|digital)([\\s:]*isbn[^\\d]*[\\dX-‒–—]+)', re.IGNORECASE),\n",
    "            re.compile(r'(isbn[\\s\\(]*)(electrónico|électronique|online|on-line|digital)([^\\d]*[\\dX-‒–—]+)', re.IGNORECASE),\n",
    "            re.compile(r'(isbn[^\\d]*[\\dX-‒–—]+[\\s\\(]*)(electrónico|électronique|online|on-line|digital)', re.IGNORECASE),\n",
    "        ],\n",
    "        'generic': [\n",
    "            re.compile(r'isbn[^\\d]*[\\dX-‒–—]+', re.IGNORECASE),\n",
    "        ],\n",
    "        'extract': {\n",
    "            'precise': re.compile(r'^[^\\d]*([\\dX-‒–—]+).*$', re.IGNORECASE),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "title_strings_csv = pkg_resources.resource_stream('isaw.awol', 'awol_title_strings.csv')\n",
    "dreader = unicodecsv.DictReader(\n",
    "    title_strings_csv,\n",
    "    fieldnames = [\n",
    "        'titles', \n",
    "        'tags'\n",
    "    ], \n",
    "    delimiter = ',', \n",
    "    quotechar = '\"')\n",
    "TITLE_SUBSTRING_TAGS = dict()\n",
    "for row in dreader:\n",
    "    TITLE_SUBSTRING_TAGS.update({row['titles']:row['tags']})\n",
    "del dreader\n",
    "TITLE_SUBSTRING_TERMS = {k:v for (k,v) in TITLE_SUBSTRING_TAGS.items() if ' ' not in k}\n",
    "TITLE_SUBSTRING_TERMS['boğazköy'] = 'Boğazköy'\n",
    "TITLE_SUBSTRING_PHRASES = {k:v for (k,v) in TITLE_SUBSTRING_TAGS.items() if k not in list(TITLE_SUBSTRING_TERMS.keys())}\n",
    "AGGREGATORS = [\n",
    "    'www.jstor.org',\n",
    "    'oi.uchicago.edu',\n",
    "    'www.persee.fr',\n",
    "    'dialnet.unirioja.es',\n",
    "    'amar.hsclib.sunysb.edu',\n",
    "    'hrcak.srce.hr',\n",
    "    'www.griffith.ox.ac.uk'\n",
    "]\n",
    "AGGREGATOR_IGNORE = [\n",
    "    'http://www.jstor.org/page/info/about/archives/collections.jsp',\n",
    "    'https://oi.uchicago.edu/getinvolved/',\n",
    "    'http://oi.uchicago.edu/news/'\n",
    "]\n",
    "POST_SELECTIVE = {\n",
    "    'http://ancientworldonline.blogspot.com/2012/07/chicago-demotic-dictionary-t.html': [0,],\n",
    "    'http://ancientworldonline.blogspot.com/2013/01/new-issues-of-asor-journals.html': [0,1,]\n",
    "}\n",
    "SUBORDINATE_FLAGS = [\n",
    "    'terms of use',\n",
    "    'download pdf',\n",
    "    'download',\n",
    "]\n",
    "NO_FORCING = [\n",
    "    'http://ancientworldonline.blogspot.com/2011/03/ancient-world-in-persee.html',\n",
    "    'http://ancientworldonline.blogspot.com/2009/09/open-access-journals-in-ancient-studies.html',\n",
    "    'http://ancientworldonline.blogspot.com/2011/05/open-access-journal-bsaa-arqueologia.html',\n",
    "]\n",
    "NO_SUBORDINATES = [\n",
    "    'http://ancientworldonline.blogspot.com/2012/12/newly-online-from-ecole-francaise-de.html',\n",
    "    'http://ancientworldonline.blogspot.com/2011/03/ancient-world-in-persee.html'\n",
    "]\n",
    "FORCE_AS_SUBORDINATE_AFTER = [\n",
    "    'http://oi.uchicago.edu/research/library/acquisitions.html',\n",
    "    'http://oi.uchicago.edu/research/pubs/ar/10-11/',\n",
    "    'http://oi.uchicago.edu/research/pubs/ar/28-59/',\n",
    "    'http://oi.uchicago.edu/research/pubs/catalog/as/',\n",
    "    'http://oi.uchicago.edu/research/pubs/catalog/as/',\n",
    "    'http://oi.uchicago.edu/research/pubs/catalog/saoc/',\n",
    "    'http://www.persee.fr/web/ouvrages/home/prescript/fond/befar',\n",
    "    'http://www.persee.fr/web/ouvrages/home/prescript/issue/mom_0184-1785_2011_act_45_1#',\n",
    "    'https://oi.uchicago.edu/research/pubs/ar/11-20/11-12/',\n",
    "    'https://oi.uchicago.edu/research/pubs/catalog/oip/',\n",
    "    'oriental institute news & notes',\n",
    "    'http://amar.hsclib.sunysb.edu/amar/',\n",
    "    'http://www.persee.fr/web/revues/home/prescript/issue/litt_0047-4800_2001_num_122_2',\n",
    "    'http://oi.uchicago.edu/research/pubs/nn/',\n",
    "    'http://ancientworldonline.blogspot.com/2010/04/open-access-journal-oriental-institute.html'\n",
    "]\n",
    "RELATED_FLAGS = [\n",
    "    'list of volumes in print',\n",
    "    'membership'\n",
    "]\n",
    "FORCE_AS_RELATED_AFTER = [\n",
    "    'http://oi.uchicago.edu/research/library/dissertation/nolan.html',\n",
    "    'http://oi.uchicago.edu/research/pubs/ar/28-59',\n",
    "    'https://oi.uchicago.edu/research/pubs/archeological/',\n",
    "    'list of volumes in print',\n",
    "]\n",
    "SUPPRESS_RESOURCE = [\n",
    "    'terms of use',\n",
    "    'download pdf',\n",
    "    'download',\n",
    "    'membership',\n",
    "    'here'\n",
    "]\n",
    "\n",
    "\n",
    "RX_DASHES = re.compile(r'[‒–—-]+')\n",
    "\n",
    "\n",
    "def clean_title(raw):\n",
    "    prepped = normalize_space(raw)\n",
    "    chopped = prepped.split('.')\n",
    "    if len(chopped) > 2:\n",
    "        cooked = '.'.join(tuple(chopped[:2]))\n",
    "        i = 2\n",
    "        while i < len(chopped) and len(cooked) < 40:\n",
    "            cooked = cooked + '.' + chopped[i]\n",
    "            i = i + 1\n",
    "    else:\n",
    "        cooked = prepped\n",
    "    junk = [\n",
    "        ('(', ')'),\n",
    "        ('[', ']'),\n",
    "        ('{', '}'),\n",
    "        ('\"', '\"'),\n",
    "        (\"'\", \"'\"),\n",
    "        ('<', '>'),\n",
    "        ('«', '»'),\n",
    "        ('‘', '’'),\n",
    "        ('‚', '‛'),\n",
    "        ('“', '”'),\n",
    "        ('‟', '„'),\n",
    "        ('‹', '›'),\n",
    "        ('〟', '＂'),\n",
    "        ('\\\\'),\n",
    "        ('/'),\n",
    "        ('|'),\n",
    "        (','),\n",
    "        (';'),\n",
    "        ('-'),\n",
    "        ('.'),\n",
    "        ('_'),\n",
    "    ]\n",
    "    for j in junk:\n",
    "        if len(j) == 2:\n",
    "            cooked = cooked[1:-1] if cooked[0] == j[0] and cooked[-1] == j[1] else cooked\n",
    "        else:\n",
    "            cooked = cooked[1:] if cooked[0] == j[0] else cooked\n",
    "            cooked = cooked[:-1] if cooked[-1] == j[0] else cooked\n",
    "        if cooked[0:4] == 'and ':\n",
    "            cooked = cooked[4:]\n",
    "        cooked = cooked.strip()\n",
    "    return cooked\n",
    "\n",
    "class AwolArticle(Article):\n",
    "    \"\"\"Manipulate and extract data from an AWOL blog post.\"\"\"\n",
    "\n",
    "    def __init__(self, atom_file_name=None, json_file_name=None):\n",
    "\n",
    "        Article.__init__(self, atom_file_name, json_file_name)\n",
    "        lt = self.title.lower()\n",
    "        if lt in list(COLON_PREFIXES.keys()):\n",
    "            if COLON_PREFIXES[lt][0] == 'yes':\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# article.py post 2to3 refactoring\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Working with blog posts.\n",
    "\n",
    "This module defines the following classes:\n",
    " \n",
    " * Article: represents key information about the post.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "from beautifulsoup4 import BeautifulSoup\n",
    "from lxml import etree as exml\n",
    "from lxml.etree import XMLSyntaxError as XMLSyntaxError\n",
    "\n",
    "from isaw.awol.normalize_space import normalize_space\n",
    "from isaw.awol.clean_string import purify_text, purify_html\n",
    "from isaw.awol.tools import urls\n",
    "\n",
    "XML_PARSER = exml.XMLParser(recover=False)\n",
    "XML_PARSER_LENIENT = exml.XMLParser(recover=True)\n",
    "XSL_CLEANUP_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'cleanup.xsl')\n",
    "XSL_CLEANUP = exml.parse(XSL_CLEANUP_PATH)\n",
    "\n",
    "class Article():\n",
    "    \"\"\"Manipulate and extract data from a blog post.\"\"\"\n",
    "\n",
    "    def __init__(self, atom_file_name=None, json_file_name=None):\n",
    "        \"\"\"Load post from Atom entry or JSON and extract basic info.\n",
    "\n",
    "        The method looks for the following components and saves their \n",
    "        values as attributes of the object:\n",
    "\n",
    "            * id (string): unique identifier for the blog post\n",
    "            * title (unicode): title of the blog post\n",
    "            * url (unicode): url of the blog post\n",
    "            * categories (list of unicode strings): categories assigned to\n",
    "              the blog post\n",
    "            * content (string): raw text content of the blog post\n",
    "            * soup: soupified content of the blog post.\n",
    "        \"\"\"\n",
    "\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "\n",
    "        if atom_file_name is not None:\n",
    "            if json_file_name is not None:\n",
    "                logger.warning(\n",
    "                    'Filenames for both Atom and JSON were specified'\n",
    "                    + ' in Article constructor. JSON filename ignored.')\n",
    "\n",
    "            self._load_atom(atom_file_name)\n",
    "        elif json_file_name is not None:\n",
    "            # todo\n",
    "            self.__load_json(json_file_name)\n",
    "\n",
    "    def _load_atom(self, atom_file_name):\n",
    "        \"\"\"Open atom file and parse for basic info.\n",
    "\n",
    "        We attempt to set the following attributes on the class:\n",
    "\n",
    "         * id (string): tag id for this atom entry\n",
    "         * title (unicode): title of the original blog post\n",
    "         * url (string): url for the original blog post)\n",
    "         * categories (dictionary) with the following keys:\n",
    "           * 'vocabulary' (string): captures \"scheme\" from the entry categories\n",
    "           * 'term' (string): verbatim from the entry categories\n",
    "         * content (unicode): normalized unicode string containing everything\n",
    "           that was in the entry content (see normalization comments below)\n",
    "         * soup (bs4 BeutifulSoup object): html-parsed version of content\n",
    "\n",
    "        All strings are space normalized (i.e., all continguous spans of\n",
    "        whitespace are collapsed to a single space and the result string is\n",
    "        stripped of leading and trailing whitespace).\n",
    "\n",
    "        The normalization form of all unicode strings (title and content) are\n",
    "        converted to Normalization Form \"C\" (canonical normalized).\n",
    "        \"\"\"\n",
    "\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "\n",
    "        with open(atom_file_name, 'r') as file_object:\n",
    "            self.doc = exml.parse(file_object)\n",
    "        self.root = self.doc.getroot()\n",
    "        root = self.root\n",
    "        self.id = root.find('{http://www.w3.org/2005/Atom}id').text.strip()\n",
    "        #logger.debug('article id: \"{0}\"'.format(self.id))\n",
    "\n",
    "        # title of blog post should be same as title of atom entry\n",
    "        raw_title = str(root.find('{http://www.w3.org/2005/Atom}title').text)\n",
    "        try:\n",
    "            self.title = purify_text(normalize_space(unicodedata.normalize('NFC', raw_title)))\n",
    "        except TypeError:\n",
    "            msg = 'could not extract blog post title for article with id: \"{0}\"'.format(self.id)\n",
    "            raise RuntimeWarning(msg)\n",
    "            \n",
    "        else:\n",
    "            #logger.debug(u'article title: \"{0}\"'.format(self.title))\n",
    "            pass\n",
    "\n",
    "        # get url of blog post (html alternate)\n",
    "        try:\n",
    "            raw_url = str(root.xpath(\"//*[local-name()='link' and @rel='alternate']\")[0].get('href'))\n",
    "        except IndexError:\n",
    "            msg = 'could not extract blog post URL for article with id: \"{0}\"'.format(self.id)\n",
    "            raise RuntimeError(msg)\n",
    "        else:\n",
    "            try:\n",
    "                raw_url = normalize_space(unicodedata.normalize('NFC', raw_url))\n",
    "            except TypeError:\n",
    "                msg = 'could not normalize blog post URL for article with id: \"{0}\"'.format(self.id)\n",
    "                raise RuntimeError(msg)\n",
    "            else:\n",
    "                if urls.valid(raw_url):\n",
    "                    self.url = raw_url\n",
    "                else:\n",
    "                    msg = 'invalid blog post URL ({0}) for article with id: \"{1}\"'.format(raw_url, self.id)\n",
    "                    raise RuntimeError(msg)\n",
    "\n",
    "        # capture categories as vocabulary terms\n",
    "        self.categories = [{'vocabulary' : c.get('scheme'), 'term' : normalize_space(unicodedata.normalize('NFC', str(c.get('term'))))} for c in root.findall('{http://www.w3.org/2005/Atom}category')]\n",
    "        \n",
    "        # extract content, normalize, and parse as HTML for later use\n",
    "        raw_content = root.find('{http://www.w3.org/2005/Atom}content').text\n",
    "        soup = BeautifulSoup(raw_content)   # mainly to convert character entities to unicode\n",
    "        soup_content = str(soup)\n",
    "        del soup\n",
    "        content = unicodedata.normalize('NFC', soup_content)\n",
    "        del soup_content\n",
    "        content = normalize_space(content)\n",
    "        content = purify_html(content)  # get rid of all manner of evil, stupid stuff\n",
    "        self.content = content\n",
    "        try:\n",
    "            html = exml.fromstring(content, XML_PARSER)\n",
    "        except XMLSyntaxError:\n",
    "            msg = 'XMLSyntaxError while trying to parse content of {0}; trying html5lib parser with BeautifulSoup and then lxml parser with recover=True'.format(atom_file_name)\n",
    "            logger.warning(msg)\n",
    "            soup = BeautifulSoup(raw_content, 'html5lib')\n",
    "            soup_content = str(soup)\n",
    "            del soup\n",
    "            content = unicodedata.normalize('NFC', soup_content)\n",
    "            del soup_content\n",
    "            content = normalize_space(content)\n",
    "            content = purify_html(content)\n",
    "            self.content = content\n",
    "            try:\n",
    "                html = exml.fromstring(content, XML_PARSER_LENIENT)\n",
    "            except XMLSyntaxError:\n",
    "                msg = 'XMLSyntaxError while trying to re-parse content of {0} using html5lib parser with BeautifulSoup'.format(atom_file_name)\n",
    "                logger.error(msg)\n",
    "                logger.error(content)\n",
    "                sys.exit(-1000)\n",
    "\n",
    "        #logger.debug('normalized html:\\n\\n' + exml.tostring(html, pretty_print=True))\n",
    "        transform = exml.XSLT(XSL_CLEANUP)\n",
    "        clean_html = transform(html)\n",
    "        #logger.debug('cleaned html:\\n\\n' + exml.tostring(clean_html, pretty_print=True))\n",
    "        self.soup = BeautifulSoup(exml.tostring(clean_html))\n",
    "\n",
    "\n",
    "    def _load_json(self, json_file_name):\n",
    "        \"\"\"open atom file and parse for basic info\"\"\"\n",
    "        emsg = 'Article constructor does not yet support JSON.'\n",
    "        raise NotImplementedError(emsg)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Print all data about the article.\"\"\"\n",
    "\n",
    "        return str(self.id+\"|\"+self.title+\"|\"+str(self.tags)+\"|\"+\n",
    "            self.content+\"|\"+self.url+\"|\"+self.blogUrl+\"|\"+self.template+\n",
    "            \"|\"+self.issn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# awol_parsers.py post 2to3 refactoring\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Bank of parsers to use for extracting AWOL blog content.\n",
    "\n",
    "This module defines the following classes:\n",
    "\n",
    " * AwolParsers: parse AWOL blog post content for resources\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from importlib import import_module\n",
    "import pkgutil\n",
    "import sys\n",
    "\n",
    "class AwolParsers():\n",
    "    \"\"\"Pluggable framework for parsing content from an AwolArticle.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Load available parsers.\"\"\"\n",
    "\n",
    "        self.parsers = {}\n",
    "\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "\n",
    "        ignore_parsers = [\n",
    "            'awol_parsers',             # self\n",
    "            'awol_parse',               # superclass\n",
    "            'awol_parse_domain',        # superclass\n",
    "        ]\n",
    "        where = 'isaw/awol/parse'\n",
    "        parser_names = [name for _, name, _ in pkgutil.iter_modules([where]) if 'parse' in name]\n",
    "        for parser_name in parser_names:\n",
    "            if parser_name not in ignore_parsers:\n",
    "                levels = where.split('/')\n",
    "                levels.append(parser_name)\n",
    "                parser_path = '.'.join(tuple(levels))\n",
    "                #logger.debug('importing module \"{0}\"'.format(parser_path))\n",
    "                mod = import_module(parser_path)\n",
    "                parser = mod.Parser()\n",
    "                self.parsers[parser.domain] = parser\n",
    "\n",
    "    def parse(self, article):\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "\n",
    "        self.reset()\n",
    "        self.content_soup = article.soup\n",
    "        domains = self.get_domains()\n",
    "        length = len(domains)\n",
    "        logger.debug(\n",
    "            '\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nparsing '\n",
    "            + article.url\n",
    "            + '\\n')\n",
    "        logger.debug('domains: {0}'.format(repr(domains)))\n",
    "        if length == 0:\n",
    "            raise NotImplementedError('awol_parsers does not know what to do with no domains in article: {0}'.format(article.id))\n",
    "        else:\n",
    "            tlow = article.title.lower()\n",
    "            if 'journal:' in tlow:\n",
    "                parser = self.parsers['generic-single']\n",
    "            elif length == 1:\n",
    "                try:\n",
    "                    parser = self.parsers[domains[0]]\n",
    "                except KeyError:\n",
    "                    if domains[0] in ['www.egyptpro.sci.waseda.ac.jp',]:\n",
    "                        parser = self.parsers['generic-single']\n",
    "                    else:\n",
    "                        parser = self.parsers['generic']\n",
    "            else:\n",
    "                raise NotImplementedError('awol_parsers does not know what to do with multiple domains in article: {0}\\n    {1}'.format(article.id, '\\n    '.join(domains)))\n",
    "            logger.info('using \"{0}\" parser'.format(parser.domain))\n",
    "            return parser.parse(article)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.content_soup = None\n",
    "\n",
    "        #for parser in self.parsers:\n",
    "        #    parser.reset()\n",
    "\n",
    "\n",
    "    def get_domains(self, content_soup=None):\n",
    "        \"\"\"find valid resource domains in content\"\"\"\n",
    "\n",
    "        if content_soup is None and self.content_soup is None:\n",
    "            raise AttributeError('No content soup has been fed to parsers.')\n",
    "\n",
    "        if content_soup is not None:\n",
    "            self.reset()\n",
    "            self.content_soup = content_soup\n",
    "\n",
    "        return self.parsers['generic'].get_domains(self.content_soup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# awol_parse_generic.py post 2to3 refactoring\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Parse HTML content for resources generically.\n",
    "\n",
    "This module defines the following classes:\n",
    "\n",
    " * Parser\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from isaw.awol.parse.awol_parse import AwolBaseParser\n",
    "\n",
    "class Parser(AwolBaseParser):\n",
    "    \"\"\"Extract data from an AWOL blog post agnostic to domain of resource.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.domain = 'generic'\n",
    "        AwolBaseParser.__init__(self)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# awol_parse.py post 2to3 refactoring\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Parse HTML content for resources.\n",
    "\n",
    "This module defines the following classes:\n",
    "\n",
    " * AwolParser: parse AWOL blog post content for resources\n",
    "\"\"\"\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "import logging\n",
    "import pkg_resources\n",
    "import pprint\n",
    "import regex as re\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "from lxml import etree\n",
    "import unicodecsv\n",
    "\n",
    "from isaw.awol.clean_string import *\n",
    "from isaw.awol.normalize_space import normalize_space\n",
    "from isaw.awol.resource import Resource\n",
    "from isaw.awol.tools import mods\n",
    "\n",
    "LANGUAGE_IDENTIFIER = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "LANGID_THRESHOLD = 0.98\n",
    "\n",
    "DOMAINS_IGNORE = [\n",
    "    'draft.blogger.com',\n",
    "    'bobcat.library.nyu.edu',\n",
    "    'www.addthis.com',\n",
    "    'cientworldonline.blogspot.com' # that there's a typo in a link somewhere in the blog\n",
    "]\n",
    "DOMAINS_SELF = [\n",
    "    'ancientworldonline.blogspot.com',\n",
    "]\n",
    "BIBLIO_SOURCES = {\n",
    "    'zenon.dainst.org': {\n",
    "        'url_pattern': re.compile('^https?:\\/\\/zenon.dainst.org/Record/\\d+\\/?$'),\n",
    "        'url_append': '/RDF',\n",
    "        'type': 'application/rdf+xml',\n",
    "        'namespaces' : {\n",
    "            'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',\n",
    "            'mods': 'http://www.loc.gov/mods/v3'\n",
    "        },\n",
    "        'payload_xpath': '//rdf:Description[1]/mods:mods[1]',\n",
    "        'payload_type': 'application/mods+xml',\n",
    "        'date_fixer': re.compile(r'^(?P<year>\\d{4})(?P<month>\\d{2})(?P<day>\\d{2})(?P<hour>\\d{2})(?P<minute>\\d{2})(?P<second>[\\d\\.]+)$')\n",
    "    }\n",
    "}\n",
    "DOMAINS_BIBLIOGRAPHIC = list(BIBLIO_SOURCES.keys())\n",
    "MODS2RESOURCES = {\n",
    "    'publisher':'publishers',\n",
    "    'language':'languages',\n",
    "    'statement_of_responsibility':'responsibility',\n",
    "    'place':'places',\n",
    "    'issued_date':'issued_dates',\n",
    "    'uri':'identifiers'\n",
    "\n",
    "}\n",
    "ANCHOR_TEXT_IGNORE = [\n",
    "    'contact us',\n",
    "]\n",
    "ANCHOR_URLS_IGNORE = [\n",
    "]\n",
    "colon_prefix_csv = pkg_resources.resource_stream('isaw.awol', 'awol_colon_prefixes.csv')\n",
    "dreader = unicodecsv.DictReader(\n",
    "    colon_prefix_csv,\n",
    "    fieldnames = [\n",
    "        'col_pre',\n",
    "        'omit_post',\n",
    "        'strip_title',\n",
    "        'mul_res'\n",
    "    ],\n",
    "    delimiter = ',',\n",
    "    quotechar = '\"')\n",
    "COLON_PREFIXES = dict()\n",
    "for row in dreader:\n",
    "    COLON_PREFIXES.update({\n",
    "        normalize_space(row['col_pre']).lower():\n",
    "            [\n",
    "                row['omit_post'],\n",
    "                row['strip_title'],\n",
    "                row['mul_res']\n",
    "            ]\n",
    "    })\n",
    "del dreader\n",
    "def check_colon(title):\n",
    "    if ':' in title:\n",
    "        colon_prefix = title.split(':')[0].lower()\n",
    "        if colon_prefix in list(COLON_PREFIXES.keys()) and (COLON_PREFIXES[colon_prefix])[1] == 'yes':\n",
    "            return clean_string(':'.join(title.split(':')[1:]))\n",
    "        else:\n",
    "            return title\n",
    "    else:\n",
    "        return title\n",
    "OMIT_TITLES = [\n",
    "    'administrative',\n",
    "    'administrative note'\n",
    "]\n",
    "def allow_by_title(title):\n",
    "    if title.lower() in OMIT_TITLES:\n",
    "        return False\n",
    "    elif ':' in title:\n",
    "        colon_prefix = title.split(':')[0].lower()\n",
    "        if colon_prefix in list(COLON_PREFIXES.keys()) and (COLON_PREFIXES[colon_prefix])[0] == 'yes':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "RX_IDENTIFIERS = {\n",
    "    'issn': {\n",
    "        'electronic': [\n",
    "            re.compile(r'(e-|e)(issn[\\s:\\-]*[\\dX\\-]{4}[\\-\\s]+[\\dX]{4})', re.IGNORECASE),\n",
    "            re.compile(r'(electronic|online|on-line|digital|internet)([\\s:]*issn[^\\d]*[\\dX]{4}[\\-\\s]+[\\dX]{4})', re.IGNORECASE),\n",
    "            re.compile(r'(issn[\\s\\(\\-]*)(electrónico|électronique|online|on-line|digital|internet)([^\\d]*[\\dX]{4}[\\-\\s]+[\\dX]{4})', re.IGNORECASE),\n",
    "            re.compile(r'(issn[^\\d]*[\\dX]{4}[\\-\\s]+[\\dX]{4}[\\s\\(]*)(electrónico|électronique|online|on-line|digital)', re.IGNORECASE),\n",
    "        ],\n",
    "        'generic': [\n",
    "            re.compile(r'(issn[^\\d]*[\\dX]{4}[\\-\\s]+[\\dX]{4})', re.IGNORECASE),\n",
    "            re.compile(r'(issn[^\\d]*[\\dX\\-\\s]{8-11})', re.IGNORECASE)\n",
    "        ],\n",
    "        'extract': {\n",
    "            'precise': re.compile(r'^[^\\d]*([\\dX]{4}[\\-\\s]+[\\dX]{4}).*$', re.IGNORECASE),\n",
    "            'fallback': re.compile(r'^[^\\d]*([\\dX\\-\\s]+).*$', re.IGNORECASE)\n",
    "        }\n",
    "    },\n",
    "    'isbn': {\n",
    "        'electronic': [\n",
    "            re.compile(r'(electronic|e-|online|on-line|digital)([\\s:]*isbn[^\\d]*[\\dX\\-]+)', re.IGNORECASE),\n",
    "            re.compile(r'(isbn[\\s\\(]*)(electrónico|électronique|online|on-line|digital)([^\\d]*[\\dX\\-]+)', re.IGNORECASE),\n",
    "            re.compile(r'(isbn[^\\d]*[\\dX\\-]+[\\s\\(]*)(electrónico|électronique|online|on-line|digital)', re.IGNORECASE),\n",
    "        ],\n",
    "        'generic': [\n",
    "            re.compile(r'isbn[^\\d]*[\\dX\\-]+', re.IGNORECASE),\n",
    "        ],\n",
    "        'extract': {\n",
    "            'precise': re.compile(r'^[^\\d]*([\\dX\\-]+).*$', re.IGNORECASE),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "RX_AUTHORS = [\n",
    "    re.compile(r'(compiled by |assembled by |created by |written by |authors?):?\\s*([^\\.]+)', re.IGNORECASE)\n",
    "]\n",
    "RX_EDITORS = [\n",
    "    re.compile(r'(edited by |editors?):?\\s*([^\\.]+)', re.IGNORECASE)\n",
    "]\n",
    "\n",
    "title_strings_csv = pkg_resources.resource_stream('isaw.awol', 'awol_title_strings.csv')\n",
    "dreader = unicodecsv.DictReader(\n",
    "    title_strings_csv,\n",
    "    fieldnames = [\n",
    "        'titles',\n",
    "        'tags'\n",
    "    ],\n",
    "    delimiter = ',',\n",
    "    quotechar = '\"')\n",
    "TITLE_SUBSTRING_TAGS = dict()\n",
    "for row in dreader:\n",
    "    TITLE_SUBSTRING_TAGS.update({row['titles']:row['tags']})\n",
    "del dreader\n",
    "TITLE_SUBSTRING_TERMS = {k:v for (k,v) in TITLE_SUBSTRING_TAGS.items() if ' ' not in k}\n",
    "TITLE_SUBSTRING_PHRASES = {k:v for (k,v) in TITLE_SUBSTRING_TAGS.items() if k not in list(TITLE_SUBSTRING_TERMS.keys())}\n",
    "RX_ANALYTIC_TITLES = [\n",
    "    # volume, issue, year (e.g. Bd. 52, Nr. 1 (2005))\n",
    "    {\n",
    "        'rx': re.compile(r'^Bd\\.\\s+(\\d+),?\\s+Nr\\.\\s+(\\d+)\\s+\\(?(\\d{4})\\)?$', re.IGNORECASE),\n",
    "        'volume': 1,\n",
    "        'issue': 2,\n",
    "        'year': 3\n",
    "    },\n",
    "    # year, blah blah blah, then volume (e.g. u'1888 Mitteilungen des Deutschen Arch\\xe4ologischen Instituts / R\\xf6mische Abteilung Band 3')\n",
    "    {\n",
    "        'rx': re.compile(r'^(\\d{4})[^\\d]+Band (\\d+)$', re.IGNORECASE),\n",
    "        'volume': 2,\n",
    "        'year': 1\n",
    "    },\n",
    "    # vol slash year (e.g. University Museums and Collections Journal 4/2011)\n",
    "    {\n",
    "        'rx': re.compile(r'^[^\\d]*(\\d+)\\/(\\d{4})[^\\d]*$', re.IGNORECASE),\n",
    "        'volume': 1,\n",
    "        'year': 2,\n",
    "    },\n",
    "    # year, then volume\n",
    "    {\n",
    "        'rx': re.compile(r'^[^\\d]*(\\d{4})\\W*([\\d\\-]+)[^\\d]*$', re.IGNORECASE),\n",
    "        'volume': 2,\n",
    "        'year': 1\n",
    "    },\n",
    "    # volume, then year\n",
    "    {\n",
    "        'rx': re.compile(r'^[^\\d]*([\\d\\-]{1-4})\\W*(\\d{4})[^\\d]*$', re.IGNORECASE),\n",
    "        'volume': 1,\n",
    "        'year': 2\n",
    "    },\n",
    "    # year only\n",
    "    {\n",
    "        'rx': re.compile(r'^[^\\d]*(\\d{4})[^\\d]*$', re.IGNORECASE),\n",
    "        'year': 1,\n",
    "    },\n",
    "    # volume only\n",
    "    {\n",
    "        'rx': re.compile(r'^[^\\d]*([\\d\\-]+)[^\\d]*$', re.IGNORECASE),\n",
    "        'volume': 1,\n",
    "    },\n",
    "\n",
    "\n",
    "]\n",
    "RX_PUNCT_FIX = re.compile(r'\\s+([\\.,:;]{1})')\n",
    "RX_PUNCT_DEDUPE = re.compile(r'([\\.,:;]{1})([\\.,:;]{1})')\n",
    "\n",
    "def domain_from_url(url):\n",
    "    return url.replace('http://', '').replace('https://', '').split('/')[0]\n",
    "\n",
    "class AwolBaseParser:\n",
    "    \"\"\"Superclass to extract resource data from an AwolArticle.\"\"\"\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    # public methods\n",
    "    def get_domains(self, content_soup=None):\n",
    "        \"\"\"Determine domains of resources linked in content.\"\"\"\n",
    "\n",
    "        #logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "\n",
    "        if content_soup is not None:\n",
    "            self.reset(content_soup)\n",
    "\n",
    "        c = self.content\n",
    "        if c['domains'] is None:\n",
    "            soup = c['soup']\n",
    "            anchors = [a for a in soup.find_all('a')]\n",
    "            urls = [a.get('href') for a in anchors if a.get('href') is not None]\n",
    "            urls = list(set(urls))\n",
    "            domains = [domain_from_url(url) for url in urls]\n",
    "            domains = list(set(domains))\n",
    "            domains = [domain for domain in domains if domain not in self.skip_domains]\n",
    "            if len(domains) > 1:\n",
    "                domains = [domain for domain in domains if domain not in self.bibliographic_domains]\n",
    "            c['domains'] = domains\n",
    "        return c['domains']\n",
    "\n",
    "    def parse(self, article):\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "        self.reset(article.soup)\n",
    "        resources = self._get_resources(article)\n",
    "        return resources\n",
    "\n",
    "    def reset(self, content_soup=None):\n",
    "        self.content = {}\n",
    "        c = self.content\n",
    "        if content_soup is not None:\n",
    "            c['soup'] = content_soup\n",
    "        c['anchors'] = None\n",
    "        c['domains'] = None\n",
    "        self.skip_domains = copy(DOMAINS_IGNORE) + copy(DOMAINS_SELF)\n",
    "        self.bibliographic_domains = copy(DOMAINS_BIBLIOGRAPHIC)\n",
    "        self.skip_text = copy(ANCHOR_TEXT_IGNORE)\n",
    "        self.skip_urls = copy(ANCHOR_URLS_IGNORE)\n",
    "\n",
    "    # private methods\n",
    "    def _consider_anchor(self, a):\n",
    "        url = a.get('href')\n",
    "        if url is not None:\n",
    "            text = a.get_text()\n",
    "            if len(text) > 0:\n",
    "                domain = domain_from_url(url)\n",
    "                if (domain in self.skip_domains\n",
    "                or url in self.skip_urls\n",
    "                or text in self.skip_text):\n",
    "                    pass\n",
    "                else:\n",
    "                    return True\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    def _filter_anchors(self, anchors):\n",
    "        filtered = [a for a in anchors if self._consider_anchor(a)]\n",
    "        return filtered\n",
    "\n",
    "    def _get_anchor_ancestor_for_title(self, anchor):\n",
    "\n",
    "        a = anchor\n",
    "        url = a.get('href')\n",
    "        parent = a.find_parent('li')\n",
    "        if parent is not None:\n",
    "            anchor_ancestor = parent\n",
    "        else:\n",
    "            previous_parent = a\n",
    "            parent = a.parent\n",
    "            while parent is not None and len([a for a in parent.find_all('a') if a.get('href') != url]) > 0:\n",
    "                prevous_parent = parent\n",
    "                parent = parent.parent\n",
    "            if previous_parent.name == 'body':\n",
    "                anchor_ancestor = anchor\n",
    "            else:\n",
    "                anchor_ancestor = previous_parent\n",
    "        return anchor_ancestor\n",
    "\n",
    "    def _get_anchors(self):\n",
    "        c = self.content\n",
    "        if c['anchors'] is not None:\n",
    "            return c['anchors']\n",
    "        soup = c['soup']\n",
    "        raw_anchors = [a for a in soup.find_all('a') if a.find_previous('a', href=a.get('href')) is None]\n",
    "        anchors = self._filter_anchors(raw_anchors)\n",
    "        c['anchors'] = anchors\n",
    "        return anchors\n",
    "\n",
    "    def _get_description(self, context=None, title=''):\n",
    "        if context is None:\n",
    "            c = self.content\n",
    "            soup = c['soup']\n",
    "            first_node = soup.body.contents[0]\n",
    "            skip_first_anchor = True\n",
    "        else:\n",
    "            first_node = context\n",
    "            skip_first_anchor = False\n",
    "\n",
    "        def digdigdig(this_node, first_node, stop_tags, skip_first_anchor, previous_urls):\n",
    "            node_type = type(this_node)\n",
    "            node_name = this_node.name\n",
    "            try:\n",
    "                node_url = this_node.get('href')\n",
    "            except AttributeError:\n",
    "                node_url = ''\n",
    "            if node_url is None:\n",
    "                node_url = ''\n",
    "            if '/' in node_url:\n",
    "                chunks = node_url.split('/')\n",
    "                if chunks[-1] in ['index.html', 'index.php', '', None]:\n",
    "                    node_url = '/'.join(chunks[:-1])\n",
    "            results = []\n",
    "            if (\n",
    "                this_node != first_node\n",
    "                and node_name in stop_tags\n",
    "                and (\n",
    "                    node_name != 'a'\n",
    "                    or (\n",
    "                        'a' in stop_tags\n",
    "                        and node_name == 'a'\n",
    "                        and (\n",
    "                                (\n",
    "                                skip_first_anchor\n",
    "                                and len(previous_urls) == 0\n",
    "                                )\n",
    "                            or (\n",
    "                                not(skip_first_anchor)\n",
    "                                and len(previous_urls) > 0\n",
    "                                and node_url != previous_urls[-1]\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                ):\n",
    "                return (True, results)\n",
    "            if node_name == 'a':\n",
    "                previous_urls.append(node_url)\n",
    "            try:\n",
    "                previous_text = normalize_space(this_node.previous_sibling.get_text())\n",
    "            except AttributeError:\n",
    "                previous_text = ''\n",
    "            try:\n",
    "                previous_last = previous_text[-1]\n",
    "            except IndexError:\n",
    "                previous_last = previous_text\n",
    "            if node_name == 'br' and previous_last != '.':\n",
    "                results.append('. ')\n",
    "            if node_type == NavigableString:\n",
    "                results.append(str(this_node))\n",
    "            else:\n",
    "                try:\n",
    "                    descendants = this_node.descendants\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                else:\n",
    "                    if descendants is not None:\n",
    "                        for child in this_node.children:\n",
    "                            stop, child_results = digdigdig(child, first_node, stop_tags, skip_first_anchor, previous_urls)\n",
    "                            results.extend(child_results)\n",
    "                            if stop:\n",
    "                                return (stop, results)\n",
    "            return (False, results)\n",
    "\n",
    "        def skiptomalou(first_node, stop_tags, skip_first_anchor):\n",
    "            previous_urls = []\n",
    "            stop, desc_lines = digdigdig(first_node, first_node, stop_tags, skip_first_anchor, previous_urls)\n",
    "            node = first_node\n",
    "            while True:\n",
    "                previous_node = node\n",
    "                node = node.next_sibling\n",
    "                if node is None:\n",
    "                    break\n",
    "                try:\n",
    "                    node_name = node.name\n",
    "                except AttributeError:\n",
    "                    node_name = type(node)\n",
    "                try:\n",
    "                    node_url = node.get('href')\n",
    "                except AttributeError:\n",
    "                    node_url = ''\n",
    "                if node_url is None:\n",
    "                    node_url = ''\n",
    "                if '/' in node_url:\n",
    "                    chunks = node_url.split('/')\n",
    "                    if chunks[-1] in ['index.html', 'index.php', '', None]:\n",
    "                        node_url = '/'.join(chunks[:-1])\n",
    "                if (\n",
    "                    node_name in stop_tags\n",
    "                    and (\n",
    "                        node_name != 'a'\n",
    "                        or (\n",
    "                            'a' in stop_tags\n",
    "                            and node_name == 'a'\n",
    "                            and (\n",
    "                                    (\n",
    "                                    not(skip_first_anchor)\n",
    "                                    and len(previous_urls) == 0\n",
    "                                    )\n",
    "                                or (\n",
    "                                    skip_first_anchor\n",
    "                                    and len(previous_urls) > 0\n",
    "                                    and node_url != previous_urls[-1]\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    ):\n",
    "                    break\n",
    "                if node_name == 'a':\n",
    "                    previous_urls.append(node_url)\n",
    "                stop, results = digdigdig(node, first_node, stop_tags, skip_first_anchor, previous_urls)\n",
    "                desc_lines.extend(results)\n",
    "                if stop:\n",
    "                    break\n",
    "            return desc_lines\n",
    "\n",
    "        stop_tags = ['a', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div', 'ol', 'ul', 'dl', 'dt', 'li', 'table']\n",
    "        desc_lines = skiptomalou(first_node, stop_tags, skip_first_anchor)\n",
    "\n",
    "        stop_tags = ['a', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "        if len(desc_lines) == 0:\n",
    "            desc_lines = skiptomalou(first_node, stop_tags, False)\n",
    "        elif ukey(desc_lines) == ukey(title):\n",
    "            desc_lines = skiptomalou(first_node, stop_tags, skip_first_anchor)\n",
    "        if len(desc_lines) == 0:\n",
    "            desc_text = None\n",
    "        else:\n",
    "            desc_text = deduplicate_lines('\\n'.join(desc_lines))\n",
    "            desc_text = ''.join(desc_lines)\n",
    "            if len(desc_text) == 0:\n",
    "                desc_text = None\n",
    "            else:\n",
    "                desc_text = desc_text.replace('%IMAGEREPLACED%', '').strip()\n",
    "                desc_text = RX_PUNCT_FIX.sub(r'\\1', desc_text)\n",
    "                desc_text = deduplicate_sentences(desc_text)\n",
    "                desc_text = RX_PUNCT_DEDUPE.sub(r'\\1', desc_text)\n",
    "                desc_text = normalize_space(desc_text)\n",
    "                if len(desc_text) == 0:\n",
    "                    desc_text = None\n",
    "                elif desc_text[-1] != '.':\n",
    "                    desc_text += '.'\n",
    "\n",
    "        return desc_text\n",
    "\n",
    "    def _get_language(self, *args):\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "        chunks = [chunk for chunk in args if chunk is not None]\n",
    "        s = ' '.join((tuple(chunks)))\n",
    "        s = normalize_space(s)\n",
    "        logger.debug('s: \\n\"{}\\n'.format(s.encode('utf-8')))\n",
    "        if s != '':\n",
    "            language = LANGUAGE_IDENTIFIER.classify(s)\n",
    "            logger.debug(repr(language))\n",
    "            if language[1] >= LANGID_THRESHOLD:\n",
    "                return language[0]\n",
    "        return None\n",
    "\n",
    "    def _get_next_valid_url(self, anchor):\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "        a = anchor\n",
    "        while a is not None:\n",
    "            logger.debug('anchor text: {0}'.format(repr(a.get_text())))\n",
    "            if a.get_text() != '':\n",
    "                try:\n",
    "                    url = a.get('href')\n",
    "                except AttributeError:\n",
    "                    url = None\n",
    "                else:\n",
    "                    domain = domain_from_url(url)\n",
    "                    if domain not in self.skip_domains:\n",
    "                        break\n",
    "            a = a.find_next('a')\n",
    "        if a is None:\n",
    "            raise ValueError('could not find valid self-or-subsequent resource anchor')\n",
    "        return (anchor, a, url, domain)\n",
    "\n",
    "    def _get_primary_anchor(self):\n",
    "        anchors = self._get_anchors()\n",
    "        try:\n",
    "            a = self._get_anchors()[0]\n",
    "        except IndexError:\n",
    "            msg = 'failed to parse primary anchor from {0}'.format(self.content['soup'])\n",
    "            raise RuntimeError(msg)\n",
    "        return a\n",
    "\n",
    "    def _get_primary_resource(self, article):\n",
    "        # title\n",
    "        a = self._get_primary_anchor()\n",
    "        a_title = clean_string(a.get_text())\n",
    "        titles = self._reconcile_titles(a_title, article.title)\n",
    "        try:\n",
    "            title = titles[0]\n",
    "        except IndexError:\n",
    "            msg = 'could not extract resource title'\n",
    "            raise IndexError(msg)\n",
    "        try:\n",
    "            title_extended = titles[1]\n",
    "        except IndexError:\n",
    "            title_extended = None\n",
    "\n",
    "        # description\n",
    "        desc_text = self._get_description(title=title)\n",
    "        if desc_text is None:\n",
    "            desc_text = title\n",
    "\n",
    "        # parse authors\n",
    "        authors = self._parse_authors(desc_text)\n",
    "\n",
    "        # parse identifiers\n",
    "        identifiers = self._parse_identifiers(desc_text)\n",
    "\n",
    "        # language\n",
    "        language = self._get_language(title, title_extended, desc_text)\n",
    "\n",
    "        # determine keywords\n",
    "        keywords = self._parse_keywords(article.title, titles[-1], article.categories)\n",
    "\n",
    "        # create and populate the resource object\n",
    "        params = {\n",
    "            'url': a.get('href'),\n",
    "            'domain': a.get('href').replace('http://', '').replace('https://', '').split('/')[0],\n",
    "            'title': title\n",
    "        }\n",
    "        if desc_text is not None:\n",
    "            params['description'] = desc_text\n",
    "        if len(authors) > 0:\n",
    "            params['authors'] = authors\n",
    "        if len(list(identifiers.keys())) > 0:\n",
    "            params['identifiers'] = identifiers\n",
    "        if title_extended is not None:\n",
    "            params['title_extended'] = title_extended\n",
    "        if language is not None:\n",
    "            params['languages'] = language\n",
    "        if len(keywords) > 0:\n",
    "            params['keywords'] = keywords\n",
    "        resource = self._make_resource(**params)\n",
    "\n",
    "        # provenance\n",
    "        self._set_provenance(resource, article)\n",
    "\n",
    "        return resource\n",
    "\n",
    "    def _get_related_resources(self):\n",
    "        resources = []\n",
    "        anchors = self._get_anchors()[1:]\n",
    "        anchors = [a for a in anchors if domain_from_url(a.get('href')) in DOMAINS_SELF]\n",
    "        for a in anchors:\n",
    "            # title\n",
    "            title_context = self._get_anchor_ancestor_for_title(a)\n",
    "            title = clean_string(title_context.get_text())\n",
    "\n",
    "            # description\n",
    "            next_node = title_context.next_element\n",
    "            desc_text = self._get_description(next_node, title=title)\n",
    "\n",
    "            # parse identifiers\n",
    "            identifiers = self._parse_identifiers(desc_text)\n",
    "\n",
    "            # language\n",
    "            language = self._get_language(title, desc_text)\n",
    "\n",
    "            # determine keywords\n",
    "            keywords = self._parse_keywords(resource_title=title, resource_text=desc_text)\n",
    "\n",
    "            # create and populate the resource object\n",
    "            r = Resource()\n",
    "            params = {\n",
    "                'url': a.get('href'),\n",
    "                'domain': a.get('href').replace('http://', '').replace('https://', '').split('/')[0],\n",
    "                'title': title\n",
    "            }\n",
    "            if desc_text is not None:\n",
    "                params['description'] = desc_text\n",
    "            if len(list(identifiers.keys())) > 0:\n",
    "                params['identifiers'] = identifiers\n",
    "            if language is not None:\n",
    "                params['languages'] = language\n",
    "            if len(keywords) > 0:\n",
    "                params['keywords'] = keywords\n",
    "            resource = self._make_resource(**params)\n",
    "            resources.append(resource)\n",
    "        return resources\n",
    "\n",
    "    def _nodesplain(self, node, gloss='', include_source=False):\n",
    "        \"\"\"Provide copious information about this XML node.\"\"\"\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "        template = \"\"\"\n",
    "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "    >>> NODESPLANATION <<<\n",
    "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "    type: {node_type}\n",
    "    name: {name}\n",
    "    xpath: /{xpath}\n",
    "    attributes: {attributes}\n",
    "    text: {text}\n",
    "    gloss: {gloss}\n",
    "    source: {source}\n",
    "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "        \"\"\"\n",
    "\n",
    "        name = node.name\n",
    "        try:\n",
    "            text = normalize_space(' '.join([string for string in node.stripped_strings]))\n",
    "        except AttributeError:\n",
    "            text = 'None'\n",
    "        try:\n",
    "            attributes = pprint.pformat(node.attrs)\n",
    "        except AttributeError:\n",
    "            attributes = 'None'\n",
    "        count = str(1+len([t for t in node.previous_siblings if t.name == name]))\n",
    "        path = ['{name}[{count}]'.format(name=name, count=count)]\n",
    "        for parent in node.parents:\n",
    "            if type(parent) != NavigableString:\n",
    "                parent_name = parent.name\n",
    "                count = str(1+len([t for t in parent.previous_siblings if t.name == parent_name]))\n",
    "            path = ['{name}[{count}]'.format(name=parent_name, count=count)] + path\n",
    "        root = [p for p in node.parents][1]\n",
    "        params = {\n",
    "            'node_type': type(node),\n",
    "            'name': name,\n",
    "            'xpath': '/'.join(path),\n",
    "            'attributes': attributes,\n",
    "            'text': text,\n",
    "            'gloss': gloss,\n",
    "            'source': root.prettify() if include_source else ''\n",
    "        }\n",
    "        return template.format(**params)\n",
    "\n",
    "    def _get_resources(self, article):\n",
    "        if allow_by_title(article.title):\n",
    "            primary_resource = self._get_primary_resource(article)\n",
    "            parent = primary_resource.package()\n",
    "            if len(list(primary_resource.identifiers.keys())) > 0:\n",
    "                try:\n",
    "                    parent['issn'] = primary_resource.identifiers['issn']['electronic'][0]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        parent['issn'] = primary_resource.identifiers['issn']['generic'][0]\n",
    "                    except KeyError:\n",
    "                        try:\n",
    "                            parent['isbn'] = primary_resource.identifiers['isbn'][0]\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "\n",
    "            subs = self._get_subordinate_resources(article, primary_resource.package())\n",
    "            for sr in subs:\n",
    "                sr.is_part_of = parent\n",
    "                primary_resource.subordinate_resources.append(sr.package())\n",
    "\n",
    "            rels = self._get_related_resources()\n",
    "            for rr in rels:\n",
    "                primary_resource.related_resources(append(rr.package()))\n",
    "\n",
    "            return [primary_resource,] + subs + rels\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _get_resource_from_article(self, article, anchor, context=None):\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "        # titles\n",
    "        anchor_title = clean_string(anchor.get_text())\n",
    "        titles = self._reconcile_titles(anchor_title, article.title)\n",
    "        try:\n",
    "            title = titles[0]\n",
    "        except IndexError:\n",
    "            msg = 'could not extract resource title'\n",
    "            raise IndexError(msg)\n",
    "        try:\n",
    "            title_extended = titles[1]\n",
    "        except IndexError:\n",
    "            title_extended = None\n",
    "\n",
    "        # description\n",
    "        desc_text = self._get_description(context, title=title)\n",
    "        if desc_text is None:\n",
    "            logger.warning('could not extract primary resource description from {0}; using title'.format(article.url))\n",
    "            desc_text = title\n",
    "\n",
    "        # parse authors\n",
    "        authors = self._parse_authors(desc_text)\n",
    "\n",
    "        # parse identifiers\n",
    "        identifiers = self._parse_identifiers(desc_text)\n",
    "\n",
    "        # language\n",
    "        language = self._get_language(title, title_extended, desc_text)\n",
    "\n",
    "        # determine keywords\n",
    "        keywords = self._parse_keywords(article.title, titles[-1], article.categories)\n",
    "\n",
    "        # create and populate the resource object\n",
    "        params = {\n",
    "            'url': anchor.get('href'),\n",
    "            'domain': domain_from_url(anchor.get('href')),\n",
    "            'title': title\n",
    "        }\n",
    "        if desc_text is not None:\n",
    "            params['description'] = desc_text\n",
    "        if len(list(identifiers.keys())) > 0:\n",
    "            params['identifiers'] = identifiers\n",
    "        if len(authors) > 0:\n",
    "            params['authors'] = authors\n",
    "        if title_extended is not None:\n",
    "            params['title_extended'] = title_extended\n",
    "        if language is not None:\n",
    "            params['languages'] = language\n",
    "        if len(keywords) > 0:\n",
    "            params['keywords'] = keywords\n",
    "        resource = self._make_resource(**params)\n",
    "\n",
    "        # provenance\n",
    "        self._set_provenance(resource, article)\n",
    "\n",
    "        return resource\n",
    "\n",
    "    def _get_resource_from_external_biblio(self, url):\n",
    "        \"\"\"Attempt to get third-party structured bibliographic data.\"\"\"\n",
    "\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "        domain = domain_from_url(url)\n",
    "\n",
    "        try:\n",
    "            biblio_howto = BIBLIO_SOURCES[domain]\n",
    "        except KeyError:\n",
    "            msg = 'parsing structured bibliographic data from {0} is not supported.'.format(domain)\n",
    "            raise NotImplementedError(msg)\n",
    "        else:\n",
    "            m = biblio_howto['url_pattern'].match(url)\n",
    "            if m:\n",
    "                biblio_url = url + biblio_howto['url_append']\n",
    "                biblio_req = requests.get(biblio_url)\n",
    "                if biblio_req.status_code == 200:\n",
    "                    actual_type = biblio_req.headers['content-type']\n",
    "                    if actual_type != biblio_howto['type']:\n",
    "                        raise IOError('got {actualtype} from {biblurl} when '\n",
    "                            + '{soughttype} was expected'.format(\n",
    "                                actualtype=actual_type,\n",
    "                                biblurl=biblio_url,\n",
    "                                soughttype=biblio_howto['type']))\n",
    "                    elif actual_type == 'application/rdf+xml':\n",
    "                        root = etree.fromstring(biblio_req.content)\n",
    "                        payload_element = root.xpath(\n",
    "                            biblio_howto['payload_xpath'],\n",
    "                            namespaces=biblio_howto['namespaces'])[0]\n",
    "                        payload = etree.tostring(payload_element, encoding='unicode')\n",
    "                    else:\n",
    "                        raise IOError('parsing content of type {actualtype} '\n",
    "                            + 'is not supported'.format(\n",
    "                                actualtype=actual_type))\n",
    "                    payload_type = biblio_howto['payload_type']\n",
    "                    if payload_type == 'application/mods+xml':\n",
    "                        biblio_data = mods.extract(payload)\n",
    "                    else:\n",
    "                        raise NotImplementedError('parsing payload of type {payloadtype} '\n",
    "                            + 'is not supported'.format(\n",
    "                                payloadtype=payload_type))\n",
    "                    params = {}\n",
    "                    for k in [k for k in list(biblio_data.keys()) if k not in ['record_change_date', 'record_creation_date', 'name']]:\n",
    "                        if k == 'uri':\n",
    "                            value = (k, biblio_data[k])\n",
    "                        elif k == 'language':\n",
    "                            value = [lang[0] for lang in biblio_data[k]]\n",
    "                        elif k == 'url':\n",
    "                            value = biblio_data[k][0]\n",
    "                            if len(biblio_data[k]) > 1:\n",
    "                                raise Exception\n",
    "                        else:\n",
    "                            value = biblio_data[k]\n",
    "                        try:\n",
    "                            rk = MODS2RESOURCES[k]\n",
    "                        except KeyError:\n",
    "                            rk = k\n",
    "                        params[rk] = value\n",
    "                    params['domain'] = domain_from_url(biblio_data['url'][0])\n",
    "                    top_resource = self._make_resource(**params)\n",
    "                    try:\n",
    "                        updated = biblio_data['record_change_date'][0]\n",
    "                    except KeyError:\n",
    "                        updated = biblio_data['record_creation_date'][0]\n",
    "                    try:\n",
    "                        rx = biblio_howto['date_fixer']\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                    else:\n",
    "                        m = rx.match(updated)\n",
    "                        if m:\n",
    "                            d = {}\n",
    "                            for k in ['year', 'month', 'day', 'hour', 'minute', 'second']:\n",
    "                                d[k] = m.group(k)\n",
    "                            logger.debug(d)\n",
    "                            updated = '{year}-{month}-{day}T{hour}:{minute}:{second}'.format(**d)\n",
    "                    resource_fields = sorted([k for k in list(params.keys()) if '_' != k[0]])\n",
    "                    top_resource.set_provenance(biblio_url, 'citesAsDataSource', updated, resource_fields)\n",
    "                    if domain == 'zenon.dainst.org':\n",
    "                        top_resource.zenon_id = url.split('/')[-1]\n",
    "                else:\n",
    "                    raise IOError(\"unsuccessfull attempt (status code {0}) \" +\n",
    "                        \"to get bibliograhic data from {1}\".format(\n",
    "                            biblio_req.status_code, biblio_url))\n",
    "            return top_resource\n",
    "\n",
    "    def _get_subordinate_resources(self, article, parent_package, start_anchor=None):\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "        resources = []\n",
    "        anchors = self._get_anchors()\n",
    "        index = 0\n",
    "        if start_anchor is not None:\n",
    "            for i,a in enumerate(anchors):\n",
    "                if a == start_anchor:\n",
    "                    index = i\n",
    "                    break\n",
    "            anchors = [a for a in anchors[index:]]\n",
    "\n",
    "        parent_domain = domain_from_url(parent_package['url'])\n",
    "        anchors = [a for a in anchors if parent_domain in a.get('href')]\n",
    "\n",
    "        for a in anchors:\n",
    "            # title\n",
    "            title_context = self._get_anchor_ancestor_for_title(a)\n",
    "            title = clean_string(title_context.get_text(' '))\n",
    "\n",
    "            # try to extract volume and year\n",
    "            try:\n",
    "                volume, issue, year = self._grok_analytic_title(title)\n",
    "            except TypeError:\n",
    "                volume = year = issue = None\n",
    "            if volume is not None and year is None and issue is not None:\n",
    "                # sometimes more than one volume falls in a single list item b/c same year or parts\n",
    "                try:\n",
    "                    parent_li = a.find_parents('li')[0]\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    try:\n",
    "                        raw = parent_li.get_text().strip()[0:4]\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            cooked = str(int(raw))\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                        else:\n",
    "                            if cooked == raw:\n",
    "                                year = cooked\n",
    "\n",
    "            # description\n",
    "            next_node = title_context.next_sibling\n",
    "            desc_text = self._get_description(next_node, title=title)\n",
    "\n",
    "            # parse identifiers\n",
    "            identifiers = self._parse_identifiers(desc_text)\n",
    "\n",
    "            # language\n",
    "            language = self._get_language(title, desc_text)\n",
    "\n",
    "            # determine keywords\n",
    "            keywords = self._parse_keywords(resource_title=title, resource_text=desc_text)\n",
    "\n",
    "            # create and populate the resource object\n",
    "            params = {\n",
    "                'url': a.get('href'),\n",
    "                'domain': a.get('href').replace('http://', '').replace('https://', '').split('/')[0],\n",
    "                'title': title,\n",
    "                'is_part_of': parent_package\n",
    "            }\n",
    "            if desc_text is not None:\n",
    "                params['description'] = desc_text\n",
    "            if len(list(identifiers.keys())) > 0:\n",
    "                params['identifiers'] = identifiers\n",
    "            if language is not None:\n",
    "                params['languages'] = language\n",
    "            if len(keywords) > 0:\n",
    "                params['keywords'] = keywords\n",
    "            if volume is not None:\n",
    "                params['volume'] = volume\n",
    "            if year is not None:\n",
    "                params['year'] = year\n",
    "            if issue is not None:\n",
    "                params['issue'] = issue\n",
    "            resource = self._make_resource(**params)\n",
    "\n",
    "            self._set_provenance(resource, article)\n",
    "\n",
    "            resources.append(resource)\n",
    "        return resources\n",
    "\n",
    "    def _get_unique_urls(self):\n",
    "        c = self.content\n",
    "        if c['unique_urls'] is not None:\n",
    "            return c['unique_urls']\n",
    "        else:\n",
    "            anchors = self._get_anchors()\n",
    "        urls = [a.get('href') for a in anchors if a.get('href') is not None]\n",
    "        unique_urls = list(set(urls))\n",
    "        c['unique_urls'] = unique_urls\n",
    "        return unique_urls\n",
    "\n",
    "    def _grok_analytic_title(self, title):\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "        for g in RX_ANALYTIC_TITLES:\n",
    "            m = g['rx'].match(title)\n",
    "            if m is not None:\n",
    "                break\n",
    "        if m is not None:\n",
    "            try:\n",
    "                volume = m.group(g['volume'])\n",
    "            except KeyError:\n",
    "                volume = None\n",
    "            try:\n",
    "                issue = m.group(g['issue'])\n",
    "            except KeyError:\n",
    "                issue = None\n",
    "            try:\n",
    "                year = m.group(g['year'])\n",
    "            except KeyError:\n",
    "                year = None\n",
    "            return (volume, issue, year)\n",
    "\n",
    "    # keyword methods\n",
    "    def _mine_keywords(self, *args):\n",
    "        tags = []\n",
    "        for s in args:\n",
    "            if s is not None:\n",
    "                lower_s = s.lower()\n",
    "                # mine for terms (i.e., single-word keys)\n",
    "                lower_list = list(set(lower_s.split()))\n",
    "                for k in list(TITLE_SUBSTRING_TERMS.keys()):\n",
    "                    if k in lower_list:\n",
    "                        tag = TITLE_SUBSTRING_TERMS[k]\n",
    "                        tags.append(tag)\n",
    "                if 'open' in lower_list and 'access' in lower_list:\n",
    "                    if 'partial' in lower_list:\n",
    "                        if 'partial open access' in lower_s:\n",
    "                            tags.append('mixed access')\n",
    "                    else:\n",
    "                        if 'open access' in lower_s:\n",
    "                            tags.append('open access')\n",
    "                if 'series' in lower_list and 'lecture' not in lower_list:\n",
    "                    tags.append('series')\n",
    "                # mine for phrases\n",
    "                for k in list(TITLE_SUBSTRING_PHRASES.keys()):\n",
    "                    if k in lower_s:\n",
    "                        tag = TITLE_SUBSTRING_PHRASES[k]\n",
    "                        tags.append(tag)\n",
    "        return tags\n",
    "\n",
    "    def _parse_keywords(self, post_title=None, resource_title=None, post_categories=[], resource_text=None):\n",
    "        \"\"\"Infer and normalize resource tags.\"\"\"\n",
    "\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "\n",
    "        # mine keywords from content\n",
    "        tags = self._mine_keywords(post_title, resource_title)\n",
    "\n",
    "        # convert post categories to tags\n",
    "        for c in post_categories:\n",
    "            tag = c['term'].lower()\n",
    "            if 'kind#post' not in tag:\n",
    "                if tag in list(TITLE_SUBSTRING_TAGS.keys()):\n",
    "                    tag = TITLE_SUBSTRING_TAGS[tag]\n",
    "                else:\n",
    "                    logger.error('unexpected category tag \"{0}\" in post with title \"{1}\"'.format(c['term'], post_title))\n",
    "                    raise Exception\n",
    "                tags.append(tag)\n",
    "        return self._clean_keywords(tags)\n",
    "\n",
    "    def _clean_keywords(self, raw_tags):\n",
    "        tags = list(set(raw_tags))\n",
    "        keywords = []\n",
    "        for tag in tags:\n",
    "            if tag == '':\n",
    "                pass\n",
    "            elif ',' in tag:\n",
    "                keywords.extend(tag.split(','))\n",
    "            else:\n",
    "                keywords.append(tag)\n",
    "        keywords = sorted([normalize_space(kw) for kw in list(set(keywords))], key=lambda s: s.lower())\n",
    "        for tag in keywords:\n",
    "            if tag == tag.upper():\n",
    "                pass\n",
    "            elif tag.lower() in list(TITLE_SUBSTRING_TAGS.keys()):\n",
    "                pass\n",
    "            elif tag != tag.lower():\n",
    "                raise ValueError('keyword \"{0}\" lacks an appropriate entry in awol_title_strings.csv'.format(tag))\n",
    "        return list(set(keywords))\n",
    "\n",
    "    def _make_resource(self, **kwargs):\n",
    "        r = Resource()\n",
    "        for k,v in list(kwargs.items()):\n",
    "            if v is not None:\n",
    "\n",
    "                if type(v) == list:\n",
    "                    value = v\n",
    "                elif type(v) in [str, str]:\n",
    "                    if k == 'url':\n",
    "                        value = v\n",
    "                    else:\n",
    "                        value = [v, ]\n",
    "                elif type(v) == tuple:\n",
    "                    value = v\n",
    "                elif type(v) == dict:\n",
    "                    value = v\n",
    "                else:\n",
    "                    value = list(v)\n",
    "                try:\n",
    "                    curv = getattr(r, k)\n",
    "                except AttributeError:\n",
    "                    raise AttributeError('{k} is not a valid attribute for a resource'.format(k=k))\n",
    "                else:\n",
    "                    if curv == None and type(value) in [str, str, dict]:\n",
    "                        setattr(r, k, value)\n",
    "                    elif curv == None:\n",
    "                        setattr(r, k, value[0])\n",
    "                        if len(value) > 1:\n",
    "                            raise Exception('rats')\n",
    "                    elif type(curv) == list:\n",
    "                        value_new = deepcopy(curv)\n",
    "                        value_new.extend(value)\n",
    "                        setattr(r, k, value_new)\n",
    "                    elif type(curv) == dict and type(value) == tuple:\n",
    "                        value_new = deepcopy(curv)\n",
    "                        value_new[value[0]] = value[1]\n",
    "                        setattr(r, k, value_new)\n",
    "                    elif type(curv) == dict and type(value) == dict:\n",
    "                        value_new = deepcopy(curv)\n",
    "                        for kk in list(value.keys()):\n",
    "                            value_new[kk] = value[kk]\n",
    "                        setattr(r, k, value_new)\n",
    "                    else:\n",
    "                        raise RuntimeError('undefined error in _make_resource()')\n",
    "\n",
    "        return r\n",
    "\n",
    "    def _parse_authors(self, content_text):\n",
    "        return self._parse_peeps(RX_AUTHORS, content_text)\n",
    "\n",
    "    def _parse_editors(self, content_text):\n",
    "        return self._parse_peeps(RX_EDITORS, content_text)\n",
    "\n",
    "    def _parse_identifiers(self, content_text):\n",
    "        \"\"\"Parse identifying strings of interest from an AWOL blog post.\"\"\"\n",
    "\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "\n",
    "        identifiers = {}\n",
    "        if content_text == None:\n",
    "            return identifiers\n",
    "        text = content_text.lower()\n",
    "        words = list(set(text.split()))\n",
    "\n",
    "        def get_candidates(k, kk, text):\n",
    "            candidates = []\n",
    "            rexx = RX_IDENTIFIERS[k]\n",
    "            for rx in rexx[kk]:\n",
    "                candidates.extend([''.join(groups) for groups in rx.findall(text)])\n",
    "            if len(candidates) > 1:\n",
    "                candidates = list(set(candidates))\n",
    "            return candidates\n",
    "\n",
    "        def extract(k, text):\n",
    "            m = RX_IDENTIFIERS[k]['extract']['precise'].match(text)\n",
    "            if m is not None:\n",
    "                if len(m.groups()) == 1:\n",
    "                    return m.groups()[0]\n",
    "            else:\n",
    "                try:\n",
    "                    m = RX_IDENTIFIERS[k]['extract']['fallback'].match(text)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                else:\n",
    "                    if m is not None:\n",
    "                        if len(m.groups()) == 1:\n",
    "                            return m.groups()[0]\n",
    "            raise Exception\n",
    "\n",
    "        for k in list(RX_IDENTIFIERS.keys()):\n",
    "            if k in ' '.join(words):\n",
    "                if k not in list(identifiers.keys()):\n",
    "                    identifiers[k] = {}\n",
    "                for kk in ['electronic', 'generic']:\n",
    "                    candidates = get_candidates(k, kk, text)\n",
    "                    if len(candidates) > 0:\n",
    "                        identifiers[k][kk] = []\n",
    "                        for candidate in candidates:\n",
    "                            extraction = extract(k, candidate)\n",
    "                            identifiers[k][kk].append(extraction)\n",
    "                        if len(identifiers[k][kk]) > 1:\n",
    "                            identifiers[k][kk] = list(set(identifiers[k][kk]))\n",
    "                if len(list(identifiers[k].keys())) == 0:\n",
    "                    logger.error('expected but failed to match valid issn in {0}'.format(text))\n",
    "                # regularize presentation form and deduplicate issns\n",
    "                if k == 'issn':\n",
    "                    try:\n",
    "                        identifiers[k]['electronic'] = [issn.replace(' ', '-').upper() for issn in identifiers[k]['electronic']]\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        identifiers[k]['generic'] = [issn.replace(' ', '-').upper() for issn in identifiers[k]['generic']]\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                    if 'electronic' in list(identifiers[k].keys()) and 'generic' in list(identifiers[k].keys()):\n",
    "                        for ident in identifiers[k]['generic']:\n",
    "                            if ident in identifiers[k]['electronic']:\n",
    "                                identifiers[k]['generic'].remove(ident)\n",
    "                        if len(identifiers[k]['generic']) == 0:\n",
    "                            del identifiers[k]['generic']\n",
    "        return identifiers\n",
    "\n",
    "    def _parse_peeps(self, rx_list, content_text):\n",
    "\n",
    "        cooked = []\n",
    "        raw = ''\n",
    "        for rx in rx_list:\n",
    "            m = rx.search(content_text)\n",
    "            if m:\n",
    "                raw = m.groups()[-1]\n",
    "                break\n",
    "        if len(raw) > 0:\n",
    "            if ',' in raw:\n",
    "                cracked = raw.split(',')\n",
    "            else:\n",
    "                cracked = [raw,]\n",
    "            for chunk in cracked:\n",
    "                if ' and ' in chunk:\n",
    "                    cooked.extend(chunk.split(' and '))\n",
    "                else:\n",
    "                    cooked.append(chunk)\n",
    "            cooked = [normalize_space(peep) for peep in cooked if len(normalize_space(peep)) > 0]\n",
    "        return cooked\n",
    "\n",
    "    def _reconcile_titles(self, anchor_title=None, article_title=None):\n",
    "\n",
    "        if anchor_title is None and article_title is None:\n",
    "            return None\n",
    "        if anchor_title is None:\n",
    "            return (check_colon(article_title),)\n",
    "        if article_title is None:\n",
    "            return (check_colon,)\n",
    "        anchor_lower = anchor_title.lower()\n",
    "        article_lower = article_title.lower()\n",
    "        if anchor_lower == article_lower:\n",
    "            return (article_title,)\n",
    "        clean_article_title = check_colon(article_title)\n",
    "        clean_article_lower = clean_article_title.lower()\n",
    "        if clean_article_lower == anchor_lower:\n",
    "            return (anchor_title,)\n",
    "        elif clean_article_lower in anchor_lower:\n",
    "            return (clean_article_title, anchor_title)\n",
    "        else:\n",
    "            return (anchor_title,)\n",
    "\n",
    "    def _set_provenance(self, resource, article, fields=None):\n",
    "        updated = article.root.xpath(\"//*[local-name()='updated']\")[0].text.strip()\n",
    "        if fields is None:\n",
    "            resource_fields = sorted([k for k in list(resource.__dict__.keys()) if '_' != k[0]])\n",
    "        else:\n",
    "            resource_fields = fields\n",
    "        resource.set_provenance(article.id, 'citesAsDataSource', updated, resource_fields)\n",
    "        resource.set_provenance(article.url, 'citesAsMetadataDocument', updated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# awol_parse_oi.py post 2to3 refactoring\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Parse HTML content for resources from the OI content aggregator.\n",
    "\n",
    "This module defines the following classes:\n",
    "\n",
    " * AwolPerseeParser: parse AWOL blog post content for resources\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from isaw.awol.parse.awol_parse_domain import AwolDomainParser\n",
    "from isaw.awol.parse.awol_parse import domain_from_url\n",
    "\n",
    "NEVER_PRIMARY_DOMAINS = [\n",
    "    'www.oxbowbooks.com',\n",
    "]\n",
    "MY_SKIP_URLS = [\n",
    "    'http://oi.uchicago.edu/news/',\n",
    "]\n",
    "\n",
    "class Parser(AwolDomainParser):\n",
    "    \"\"\"Extract data from an AWOL blog post about content on OI.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.domain = 'oi.uchicago.edu'\n",
    "        AwolDomainParser.__init__(self)\n",
    "        \n",
    "    def _get_primary_anchor(self):\n",
    "        \"\"\"Deal with OI peculiarities.\"\"\"\n",
    "        for pa in AwolDomainParser._get_anchors(self):\n",
    "            url = pa.get('href')\n",
    "            domain = domain_from_url(url)\n",
    "            if domain not in NEVER_PRIMARY_DOMAINS and url not in MY_SKIP_URLS:\n",
    "                break\n",
    "        return pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resource.py post 2to3 refactoring\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Define classes and methods for working with resources extracted from blog.\n",
    "\n",
    "This module defines the following classes:\n",
    "\n",
    " * Resource: Extracts and represents key information about a web resource.\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "from wikidata_suggest import suggest\n",
    "\n",
    "PROVENANCE_VERBS = {\n",
    "    'citesAsMetadataDocument': 'http://purl.org/spar/cito/citesAsMetadataDocument',\n",
    "    'citesAsDataSource': 'http://purl.org/spar/cito/citesAsDataSource',\n",
    "    'hasWorkflowMotif': 'http://purl.org/net/wf-motifs#hasWorkflowMotif',\n",
    "    'Combine': 'http://purl.org/net/wf-motifs#Combine'\n",
    "}\n",
    "\n",
    "class Resource:\n",
    "    \"\"\"Store, manipulate, and export data about a single information resource.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Set all attributes to default values.\"\"\"\n",
    "\n",
    "        self.authors = []\n",
    "        self.contributors = []\n",
    "        self.description = None\n",
    "        self.domain = None\n",
    "        self.editors = []\n",
    "        self.end_date = None\n",
    "        self.extent = None\n",
    "        self.form = None\n",
    "        self.frequency = None\n",
    "        self.identifiers = {}\n",
    "        self.is_part_of = None\n",
    "        self.issue = None\n",
    "        self.issuance = None\n",
    "        self.issued_dates = None\n",
    "        self.keywords = []\n",
    "        self.languages = []\n",
    "        self.places = []\n",
    "        self.provenance = []\n",
    "        self.publishers = []\n",
    "        self.related_resources = []\n",
    "        self.responsibility = []\n",
    "        self.start_date = None\n",
    "        self.subordinate_resources = []\n",
    "        self.title = None\n",
    "        self.title_alternates = []\n",
    "        self.title_extended = None\n",
    "        self.type = None\n",
    "        self.url = None\n",
    "        self.url_alternates = []\n",
    "        self.volume = None\n",
    "        self.year = None\n",
    "        self.zenon_id = None\n",
    "        self.zotero_id = None\n",
    "\n",
    "    def json_dumps(self, formatted=False):\n",
    "        \"\"\"Dump resource to JSON as a UTF-8 string.\"\"\"\n",
    "\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "        dump = self.__dict__.copy()\n",
    "        for k,v in dump.items():\n",
    "            logger.debug(\"{0} ({1})\".format(k, type(v)))\n",
    "        if formatted:\n",
    "            return json.dumps(dump, indent=4, sort_keys=True, ensure_ascii=False).encode('utf8')\n",
    "        else:\n",
    "            return json.dumps(dump, ensure_ascii=False).encode('utf8')\n",
    "\n",
    "    def json_dump(self, filename, formatted=False):\n",
    "        \"\"\"Dump resource as JSON to a UTF-8 encoded file.\"\"\"\n",
    "        dumps = self.json_dumps(formatted) # get utf8-encoded JSON dump\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(dumps)\n",
    "        del dumps\n",
    "\n",
    "\n",
    "    def json_loads(self, s):\n",
    "        \"\"\"Parse resource from a UTF-8 JSON string.\"\"\"\n",
    "        self.__dict__ = json.loads(str(s))\n",
    "\n",
    "    def json_load(self, filename):\n",
    "        \"\"\"Parse resource from a json file.\"\"\"\n",
    "        with io.open(filename, 'r', encoding='utf8') as f:\n",
    "            self.__dict__ = json.load(f)\n",
    "\n",
    "    def package(self):\n",
    "        \"\"\"Return a summary package of resource information.\"\"\"\n",
    "        pkg = {}\n",
    "        try:\n",
    "            title = self.extended_title\n",
    "        except AttributeError:\n",
    "            title = self.title\n",
    "        pkg['title_full'] = title\n",
    "        pkg['url'] = self.url\n",
    "        if title != self.title:\n",
    "            pkg['title'] = self.title\n",
    "        return pkg\n",
    "\n",
    "\n",
    "    def zotero_add(self, zot, creds, extras={}):\n",
    "        \"\"\"Upload as a record to Zotero.\"\"\"\n",
    "\n",
    "        logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "\n",
    "        try:\n",
    "            issn = self.identifiers['issn']\n",
    "        except KeyError:\n",
    "            if 'journal' in self.keywords:\n",
    "                zot_type = 'journalArticle'\n",
    "            else:\n",
    "                zot_type = 'webpage'\n",
    "        else:\n",
    "            zot_type = 'journalArticle'\n",
    "        template = zot.item_template(zot_type)\n",
    "        template['abstractNote'] = self.description\n",
    "        if 'issn' in locals():\n",
    "            template['issn'] = issn\n",
    "        template['tags'] = self.keywords\n",
    "        template['extra'] = ', '.join([':'.join((k,'\"{0}\"'.format(v))) for k,v in extras.items()])\n",
    "        try:\n",
    "            template['language'] = self.language[0]\n",
    "        except TypeError:\n",
    "            pass\n",
    "        template['title'] = self.title\n",
    "        template['url'] = self.url\n",
    "        resp = zot.create_items([template])\n",
    "        try:\n",
    "            zot_id = resp['success']['0']\n",
    "            logger.debug(\"zot_id: {0}\".format(zot_id))\n",
    "        except KeyError:\n",
    "            logger.error('Zotero upload appears to have failed with {0}'.format(repr(resp)))\n",
    "            raise\n",
    "        else:\n",
    "            self.zotero_id = {\n",
    "                'libraryType': creds['libraryType'],\n",
    "                'libraryID': creds['libraryID'],\n",
    "                'itemID': zot_id\n",
    "            }\n",
    "            logger.debug(repr(self.zotero_id))\n",
    "\n",
    "    def wikidata_suggest(self, resource_title):\n",
    "        wikidata = suggest(resource_title)\n",
    "        if wikidata:\n",
    "            return wikidata['id']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def set_provenance(self, object, verb='citesAsMetadataDocument', object_date=None, fields=None):\n",
    "        \"\"\"Add an entry to the provenance list.\"\"\"\n",
    "\n",
    "        d = {\n",
    "            'term': PROVENANCE_VERBS[verb],\n",
    "            'when': datetime.datetime.utcnow().isoformat(),\n",
    "            'resource': object\n",
    "        }\n",
    "        if object_date is not None:\n",
    "            d['resource_date'] = object_date\n",
    "        if fields is not None:\n",
    "            if fields is list:\n",
    "                d['fields'] = fields\n",
    "            else:\n",
    "                d['fields'] = list(fields)\n",
    "        self.provenance.append(d)\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.__dict__, indent=4, width=120)\n",
    "\n",
    "\n",
    "def merge(r1, r2):\n",
    "    \"\"\"Merge two resources into oneness.\"\"\"\n",
    "    logger = logging.getLogger(sys._getframe().f_code.co_name)\n",
    "    r3 = Resource()\n",
    "    modified_fields = []\n",
    "    k1 = list(r1.__dict__.keys())\n",
    "    k2 = list(r2.__dict__.keys())\n",
    "    all_keys = list(set(k1 + k2))\n",
    "    domain = r1.domain\n",
    "    for k in all_keys:\n",
    "        modified = False\n",
    "        v3 = None\n",
    "        try:\n",
    "            v1 = copy.deepcopy(r1.__dict__[k])\n",
    "        except KeyError:\n",
    "            v1 = None\n",
    "        try:\n",
    "            v2 = copy.deepcopy(r2.__dict__[k])\n",
    "        except KeyError:\n",
    "            v2 = None\n",
    "\n",
    "        if k in ['url',]:\n",
    "            if v1 != v2:\n",
    "                if v1.startswith(v2):\n",
    "                    v3 = v2\n",
    "                    r3.__dict__['url_alternates'].append(v1)\n",
    "                elif v2.startswith(v1):\n",
    "                    v3 = v1\n",
    "                    r3.__dict__['url_alternates'].append(v2)\n",
    "                else:\n",
    "                    protocol1, path1 = v1.split('://')\n",
    "                    protocol2, path2 = v2.split('://')\n",
    "                    if path1 == path2 and (protocol1 == 'https' or protocol2 == 'https'):\n",
    "                        v3 = 'https://' + path1\n",
    "                    else:\n",
    "                        raise ValueError('could not reconcile url mismatch in merge: {1} vs. {2}'.format(k, v1, v2))\n",
    "            else:\n",
    "                v3 = v1\n",
    "        else:\n",
    "            modified = True\n",
    "            if v1 is None and v2 is None:\n",
    "                v3 = None\n",
    "                modified = False\n",
    "            # prefer some data over no data\n",
    "            elif v1 is None and v2 is not None:\n",
    "                v3 = v2\n",
    "            elif v1 is not None and v2 is None:\n",
    "                v3 = v1\n",
    "            elif k == 'is_part_of':\n",
    "                if v1 == v2:\n",
    "                    v3 = v1\n",
    "                    modified = False\n",
    "                else:\n",
    "                    if domain in v1['url']:\n",
    "                        v3 = v1\n",
    "                    elif domain in v2['url']:\n",
    "                        v3 = v2\n",
    "                    elif 'issn' in list(v1.keys()) and not('issn' in list(v2.keys())):\n",
    "                        v3 = v1\n",
    "                    elif 'issn' in list(v2.keys()) and not('issn' in list(v1.keys())):\n",
    "                        v3 = v2\n",
    "                    else:\n",
    "                        v3 = None\n",
    "            elif k in ['volume', 'year', 'zenon_id', 'issue', 'zotero_id']:\n",
    "                if v1 == v2:\n",
    "                    v3 = v1\n",
    "                    modified = False\n",
    "                elif v1 is None and v1 is not None:\n",
    "                    v3 = v2\n",
    "                elif v1 is not None and v2 is None:\n",
    "                    v3 = v1\n",
    "                else:\n",
    "                    raise ValueError('cannot merge two resources in which the {0} field differs: \"{1}\" vs. \"{2}\"'.format(k, v1, v2))\n",
    "            elif k == 'languages':\n",
    "                if len(v1) == 0 and len(v2) > 0:\n",
    "                    v3 = copy.deepcopy(v2)\n",
    "                elif len(v1) > 0 and len(v2) == 0:\n",
    "                    v3 = copy.deepcopy(v1)\n",
    "                elif len(v1) > 0 and len(v2) > 0:\n",
    "                    v3 = list(set(v1 + v2))\n",
    "                else:\n",
    "                    v3 = []\n",
    "            elif k == 'identifiers':\n",
    "                if len(v1) == 0 and len(v2) > 0:\n",
    "                    v3 = copy.deepcopy(v2)\n",
    "                elif len(v1) > 0 and len(v2) == 0:\n",
    "                    v3 = copy.deepcopy(v1)\n",
    "                elif len(v1) > 0 and len(v2) > 0:\n",
    "                    v3 = {}\n",
    "                    idfams = list(set(list(v1.keys()) + list(v2.keys())))\n",
    "                    for idfam in idfams:\n",
    "                        thisval1 = None\n",
    "                        thisval2 = None\n",
    "                        try:\n",
    "                            thisval1 = v1[idfam]\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "                        try:\n",
    "                            thisval2 = v2[idfam]\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "                        if type(thisval1) == list or type(thisval2) == list:\n",
    "                            v3[idfam] = []\n",
    "                            if thisval1 is not None:\n",
    "                                v3[idfam].extend(thisval1)\n",
    "                            if thisval2 is not None:\n",
    "                                v3[idfam].extend(thisval2)\n",
    "                            v3[idfam] = list(set(v3[idfam]))\n",
    "                        elif type(thisval1) == dict or type(thisval2) == dict:\n",
    "                            if thisval1 is None and thisval2 is not None:\n",
    "                                v3 = copy.deepcopy(v2)\n",
    "                            elif thisval1 is not None and thisval2 is None:\n",
    "                                v3 = copy.deepcopy(v1)\n",
    "                            else:\n",
    "                                v3[idfam] = {}\n",
    "                                idtypes = list(set(list(thisval1.keys()) + list(thisval2.keys())))\n",
    "                                for idtype in idtypes:\n",
    "                                    thissubval1 = None\n",
    "                                    thissubval2 = None\n",
    "                                    try:\n",
    "                                        thissubval1 = v1[idfam][idtype]\n",
    "                                    except KeyError:\n",
    "                                        pass\n",
    "                                    try:\n",
    "                                        thissubval2 = v2[idfam][idtype]\n",
    "                                    except KeyError:\n",
    "                                        pass\n",
    "                                    v3[idfam][idtype] = []\n",
    "                                    if thissubval1 is not None:\n",
    "                                        v3[idfam][idtype].extend(thissubval1)\n",
    "                                    if thissubval2 is not None:\n",
    "                                        v3[idfam][idtype].extend(thissubval2)\n",
    "                                    v3[idfam][idtype] = list(set(v3[idfam][idtype]))\n",
    "                else:\n",
    "                    v3 = {}\n",
    "\n",
    "            elif k in ['subordinate_resources', 'related_resources']:\n",
    "                if len(v1) == 0 and len(v2) == 0:\n",
    "                    modified = False\n",
    "                v3 = v1 + v2\n",
    "                seen = []\n",
    "                for v3_child in v3:\n",
    "                    if v3_child['url'] in seen:\n",
    "                        del(v3_child)\n",
    "                    else:\n",
    "                        seen.append(v3_child['url'])\n",
    "                del(seen)\n",
    "            elif k == 'provenance':\n",
    "                modified = False\n",
    "                v3 = v1 + v2\n",
    "            elif type(v1) == list and type(v2) == list:\n",
    "                if len(v1) == 0 and len(v2) == 0:\n",
    "                    modified = False\n",
    "                    v3 = []\n",
    "                elif len(v1) == 0 and len(v2) > 0:\n",
    "                    v3 = v2\n",
    "                elif len(v1) > 0 and len(v2) == 0:\n",
    "                    v3 = v1\n",
    "                else:\n",
    "                    v3 = list(set(v1 + v2))\n",
    "            elif type(v1) in [str, str]:\n",
    "                if len(v1) == 0 and len(v2) == 0:\n",
    "                    modified = False\n",
    "                    v3 = v1\n",
    "                elif v1 == v2:\n",
    "                    modified = False\n",
    "                    v3 = v1\n",
    "                # if one contains the other, prefer the container\n",
    "                elif v1 in v2:\n",
    "                    v3 = v2\n",
    "                elif v2 in v1:\n",
    "                    v3 = v1\n",
    "                # prefer the longer of the two\n",
    "                elif len(v1) > len(v2):\n",
    "                    v3 = v1\n",
    "                else:\n",
    "                    v3 = v2\n",
    "            else:\n",
    "                raise Exception\n",
    "        r3.__dict__[k] = v3\n",
    "        if modified:\n",
    "            modified_fields.append(k)\n",
    "    r3.set_provenance('http://purl.org/net/wf-motifs#Combine', 'hasWorkflowMotif', fields=modified_fields)\n",
    "    return r3\n",
    "\n",
    "\n",
    "def scriptinfo():\n",
    "    '''\n",
    "    Returns a dictionary with information about the running top level Python\n",
    "    script:\n",
    "    ---------------------------------------------------------------------------\n",
    "    dir:    directory containing script or compiled executable\n",
    "    name:   name of script or executable\n",
    "    source: name of source code file\n",
    "    ---------------------------------------------------------------------------\n",
    "    \"name\" and \"source\" are identical if and only if running interpreted code.\n",
    "    When running code compiled by py2exe or cx_freeze, \"source\" contains\n",
    "    the name of the originating Python script.\n",
    "    If compiled by PyInstaller, \"source\" contains no meaningful information.\n",
    "    '''\n",
    "\n",
    "    import os, sys, inspect\n",
    "    #---------------------------------------------------------------------------\n",
    "    # scan through call stack for caller information\n",
    "    #---------------------------------------------------------------------------\n",
    "    for teil in inspect.stack():\n",
    "        # skip system calls\n",
    "        if teil[1].startswith(\"<\"):\n",
    "            continue\n",
    "        if teil[1].upper().startswith(sys.exec_prefix.upper()):\n",
    "            continue\n",
    "        trc = teil[1]\n",
    "\n",
    "    # trc contains highest level calling script name\n",
    "    # check if we have been compiled\n",
    "    if getattr(sys, 'frozen', False):\n",
    "        scriptdir, scriptname = os.path.split(sys.executable)\n",
    "        return {\"dir\": scriptdir,\n",
    "                \"name\": scriptname,\n",
    "                \"source\": trc}\n",
    "\n",
    "    # from here on, we are in the interpreted case\n",
    "    scriptdir, trc = os.path.split(trc)\n",
    "    # if trc did not contain directory information,\n",
    "    # the current working directory is what we need\n",
    "    if not scriptdir:\n",
    "        scriptdir = os.getcwd()\n",
    "\n",
    "    scr_dict ={\"name\": trc,\n",
    "               \"source\": trc,\n",
    "               \"dir\": scriptdir}\n",
    "    return scr_dict\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
